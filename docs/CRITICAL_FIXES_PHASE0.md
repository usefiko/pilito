# ğŸš¨ Critical Fixes - Phase 0 (Immediate)

## ğŸ“‹ **Overview:**

Ø¨Ø¹Ø¯ Ø§Ø² review ØªÙˆØ³Ø· AI Ø¯ÛŒÚ¯Ù‡ØŒ 2 ØªØ§ Ù…Ø´Ú©Ù„ **Critical** Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù† Ú©Ù‡ Ø¨Ø§ÛŒØ¯ **Ø§Ù„Ø§Ù†** fix Ø¨Ø´Ù†:

1. âŒ Race Condition: No unique constraint on TenantKnowledge
2. âŒ Thundering Herd: Chunking tasks Ù‡Ù…Ø²Ù…Ø§Ù† fire Ù…ÛŒØ´Ù†

---

## ğŸ”§ **Fix #1: Add Unique Constraint**

### **Problem:**
```python
# 2 worker Ù…ÛŒØªÙˆÙ†Ù† Ù‡Ù…Ø²Ù…Ø§Ù† duplicate chunk Ø¨Ø³Ø§Ø²Ù†
# Ù‡ÛŒÚ† constraint Ù†Ø¯Ø§Ø±ÛŒÙ… Ø±ÙˆÛŒ (user, source_id, chunk_type)
```

### **Solution:**

**File:** `src/AI_model/models.py`

```python
class TenantKnowledge(models.Model):
    # ... existing fields ...
    
    class Meta:
        db_table = 'tenant_knowledge'
        verbose_name = "ğŸ“š Tenant Knowledge (RAG)"
        verbose_name_plural = "ğŸ“š Tenant Knowledge (RAG)"
        indexes = [
            models.Index(fields=['user', 'chunk_type']),
            models.Index(fields=['user', 'document_id']),
            models.Index(fields=['created_at']),
        ]
        # âœ… NEW: Prevent duplicate chunks
        constraints = [
            models.UniqueConstraint(
                fields=['user', 'source_id', 'chunk_type'],
                condition=models.Q(source_id__isnull=False),  # Only when source_id exists
                name='unique_chunk_per_source',
                violation_error_message='Ø§ÛŒÙ† ØµÙØ­Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ chunk Ø´Ø¯Ù‡ Ø§Ø³Øª'
            )
        ]
```

### **Migration:**

**File:** `src/AI_model/migrations/0010_add_unique_constraint.py`

```python
from django.db import migrations, models

class Migration(migrations.Migration):

    dependencies = [
        ('AI_model', '0009_add_parent_child_chunks'),
    ]

    operations = [
        migrations.AddConstraint(
            model_name='tenantknowledge',
            constraint=models.UniqueConstraint(
                fields=['user', 'source_id', 'chunk_type'],
                condition=models.Q(source_id__isnull=False),
                name='unique_chunk_per_source',
                violation_error_message='Ø§ÛŒÙ† ØµÙØ­Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ chunk Ø´Ø¯Ù‡ Ø§Ø³Øª'
            ),
        ),
    ]
```

### **Update Chunking Logic:**

**File:** `src/AI_model/services/incremental_chunker.py`

```python
def chunk_webpage(self, page) -> bool:
    try:
        from django.db import IntegrityError
        # ... existing code ...
        
        # âœ… Handle duplicate gracefully
        if chunks_to_create:
            try:
                TenantKnowledge.objects.bulk_create(
                    chunks_to_create,
                    batch_size=100,
                    ignore_conflicts=True  # âœ… Skip duplicates
                )
                logger.info(f"âœ… Created {len(chunks_to_create)} chunks")
            except IntegrityError as e:
                logger.warning(f"âš ï¸ Some chunks already exist: {e}")
                # Try individual inserts for the ones that don't exist
                success_count = 0
                for chunk in chunks_to_create:
                    try:
                        chunk.save()
                        success_count += 1
                    except IntegrityError:
                        pass  # Skip duplicate
                logger.info(f"âœ… Created {success_count}/{len(chunks_to_create)} new chunks")
        
        return True
```

---

## ğŸ”§ **Fix #2: Stagger Chunking Tasks (Prevent Thundering Herd)**

### **Problem:**
```python
# Ø¨Ø¹Ø¯ Ø§Ø² crawl 200 pageØŒ Ù‡Ù…Ù‡ Ù‡Ù…Ø²Ù…Ø§Ù† Ø¨Ø±Ø§ÛŒ chunking queue Ù…ÛŒØ´Ù†
# â†’ Redis/Celery overload
```

### **Solution:**

**File:** `src/AI_model/signals.py`

```python
import random
from django.db.models.signals import post_save
from django.dispatch import receiver
import logging

logger = logging.getLogger(__name__)

@receiver(post_save, sender='web_knowledge.WebsitePage')
def on_webpage_saved_for_chunking(sender, instance, **kwargs):
    """
    Auto-chunk WebPage when processing completes (with staggered dispatch)
    
    Improvements:
    - Random delay (10-60s) to prevent thundering herd
    - Check if already chunked (idempotent)
    - Graceful handling of duplicates
    """
    if instance.processing_status != 'completed':
        return
    
    # Check if already chunked (idempotent)
    from AI_model.models import TenantKnowledge
    already_chunked = TenantKnowledge.objects.filter(
        source_id=instance.id,
        chunk_type='website'
    ).exists()
    
    if already_chunked:
        logger.debug(f"WebPage {instance.id} already chunked, skipping")
        return
    
    from AI_model.tasks import chunk_webpage_async
    
    # âœ… Stagger tasks: Random delay 10-60 seconds
    # This prevents 200 tasks from hitting Celery/Redis simultaneously
    countdown = random.randint(10, 60)
    
    chunk_webpage_async.apply_async(
        args=[str(instance.id)],
        countdown=countdown,
        retry=False  # Don't auto-retry (signal will fire again if needed)
    )
    
    logger.debug(f"Queued chunking for WebPage {instance.id} (delay: {countdown}s)")
```

---

## ğŸ”§ **Fix #3: Stagger Processing Tasks (Ø¯Ø± crawl_website_task)**

**File:** `src/web_knowledge/tasks.py`

```python
# Line 103-136 (Ø¯Ø± crawl_website_task)
for i, page_data in enumerate(crawled_pages):
    try:
        # ... save page logic ...
        
        # âœ… Stagger processing: 200 pages over 3-5 minutes
        # Formula: (i * 1.5) seconds = 0, 1.5, 3, 4.5, ... 297s (for 200 pages)
        countdown = int(i * 1.5)  # Spread over ~5 minutes
        
        process_page_content_task.apply_async(
            args=[str(page.id)],
            countdown=countdown
        )
        
        saved_pages += 1
        
        if (i + 1) % 50 == 0:
            logger.info(f"Queued {i + 1}/{len(crawled_pages)} processing tasks")
            
    except Exception as e:
        logger.error(f"Error queuing page {page_data.get('url')}: {str(e)}")
        failed_pages += 1

logger.info(
    f"âœ… All {len(crawled_pages)} tasks queued "
    f"(staggered over ~{int(len(crawled_pages) * 1.5 / 60)} minutes)"
)
```

---

## ğŸ“Š **Impact Analysis:**

### **Before (Ø§Ù„Ø§Ù†):**
```
Crawl 200 pages:
  - 200 process tasks â†’ instant dispatch â†’ thundering herd
  - 200 chunk tasks â†’ instant dispatch â†’ thundering herd
  - Redis queue spike: 0 â†’ 400 â†’ 0 (in <5s)
  - Potential: Connection pool exhaustion, task loss
```

### **After (Ø¨Ø§ fixes):**
```
Crawl 200 pages:
  - 200 process tasks â†’ staggered 0-300s â†’ smooth queue
  - 200 chunk tasks â†’ random 10-60s â†’ distributed load
  - Redis queue: Gradual fill, no spike
  - Unique constraint: No duplicate chunks
```

### **Performance:**
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Redis Peak Load** | 400 tasks | ~50-80 tasks | 5x smoother |
| **Duplicate Chunks** | Possible | Prevented | 100% |
| **Total Time** | ~5 min | ~5-6 min | +1min (acceptable) |
| **Stability** | Risk of crash | Stable | âœ… |

---

## ğŸš€ **Deployment Steps:**

### **Step 1: Apply Migration**
```bash
cd ~/pilito
git pull origin main

# Create migration
docker-compose exec web python manage.py makemigrations AI_model

# Apply migration (this will add unique constraint)
docker-compose exec web python manage.py migrate AI_model
```

### **Step 2: Rebuild & Restart**
```bash
docker-compose build web
docker-compose up -d web
docker-compose restart celery_worker celery_ai
```

### **Step 3: Test**
```bash
# Check constraint exists
docker-compose exec db psql -U pilito -d pilito -c "
SELECT conname, pg_get_constraintdef(oid) 
FROM pg_constraint 
WHERE conname = 'unique_chunk_per_source';
"

# Should see: unique_chunk_per_source | UNIQUE (user_id, source_id, chunk_type) WHERE ...
```

### **Step 4: Monitor**
```bash
# Watch queue size (should not spike)
watch -n 2 'docker-compose exec redis redis-cli LLEN low_priority'

# Watch for duplicate errors (should be handled gracefully)
docker-compose logs -f celery_worker | grep "already exist"
```

---

## âš ï¸ **Rollback Plan:**

Ø§Ú¯Ø± Ù…Ø´Ú©Ù„ÛŒ Ù¾ÛŒØ´ Ø§ÙˆÙ…Ø¯:

```bash
# 1. Remove constraint
docker-compose exec web python manage.py dbshell
# Ø¯Ø± psql:
ALTER TABLE tenant_knowledge DROP CONSTRAINT IF EXISTS unique_chunk_per_source;

# 2. Revert code
git revert HEAD
docker-compose up -d web celery_worker celery_ai
```

---

## ğŸ“ **Next Steps (Phase 1):**

Ø¨Ø¹Ø¯ Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Phase 0 stable Ø´Ø¯:

1. âœ… Bulk DB operations (instead of loop)
2. âœ… Batch embedding API calls
3. âœ… Redis-based circuit breaker
4. âœ… Enhanced monitoring

---

## âœ… **Testing Checklist:**

- [ ] Unique constraint added successfully
- [ ] No duplicate chunks created
- [ ] Staggered dispatch working (check Redis queue)
- [ ] No thundering herd spike
- [ ] Existing pages still chunk correctly
- [ ] Error handling for duplicates works
- [ ] Performance acceptable (~1min slower, but stable)

---

**Ø²Ù…Ø§Ù† ØªØ®Ù…ÛŒÙ†ÛŒ Ø§Ø¬Ø±Ø§:** 30 Ø¯Ù‚ÛŒÙ‚Ù‡
**Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ:** Low (ÙÙ‚Ø· migration + 2 file edit)
**Ø±ÛŒØ³Ú©:** Very Low (backward compatible)

