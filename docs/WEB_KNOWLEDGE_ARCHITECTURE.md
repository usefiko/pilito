# ğŸŒ Web Knowledge System - Complete Architecture Guide

**Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¬Ø§Ù…Ø¹ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø³ÛŒØ³ØªÙ… Ø¯Ø§Ù†Ø´ Ø§Ø² ÙˆØ¨**

> **Ù†Ø³Ø®Ù‡:** 2.0  
> **ØªØ§Ø±ÛŒØ®:** October 2025  
> **ÙˆØ¶Ø¹ÛŒØª:** Production-Ready

---

## ğŸ“‹ ÙÙ‡Ø±Ø³Øª Ù…Ø·Ø§Ù„Ø¨

1. [Ù†Ù…Ø§ÛŒ Ú©Ù„ÛŒ Ø³ÛŒØ³ØªÙ…](#1-Ù†Ù…Ø§ÛŒ-Ú©Ù„ÛŒ-Ø³ÛŒØ³ØªÙ…)
2. [Ù…Ø¹Ù…Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„](#2-Ù…Ø¹Ù…Ø§Ø±ÛŒ-Ú©Ø§Ù…Ù„)
3. [ÙØ§Ø² 1: Web Crawling (Ø®Ø²ÛŒØ¯Ù† ÙˆØ¨)](#3-ÙØ§Ø²-1-web-crawling)
4. [ÙØ§Ø² 2: AI Analysis (Ø¢Ù†Ø§Ù„ÛŒØ² Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ)](#4-ÙØ§Ø²-2-ai-analysis)
5. [ÙØ§Ø² 3: Knowledge Storage (Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ù†Ø´)](#5-ÙØ§Ø²-3-knowledge-storage)
6. [ÙØ§Ø² 4: Serving with OpenAI (Ø³Ø±Ùˆ Ø¨Ø§ OpenAI)](#6-ÙØ§Ø²-4-serving-with-openai)
7. [Q&A System (Ø³ÛŒØ³ØªÙ… Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨)](#7-qa-system)
8. [Products System (Ø³ÛŒØ³ØªÙ… Ù…Ø­ØµÙˆÙ„Ø§Øª)](#8-products-system)
9. [API Endpoints](#9-api-endpoints)
10. [Database Schema](#10-database-schema)
11. [Performance & Optimization](#11-performance--optimization)

---

## 1. Ù†Ù…Ø§ÛŒ Ú©Ù„ÛŒ Ø³ÛŒØ³ØªÙ…

### **ğŸ¯ Ù‡Ø¯Ù**
ØªØ¨Ø¯ÛŒÙ„ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø­ØªÙˆØ§ÛŒ ÙˆØ¨Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ Ø¨Ù‡ ÛŒÚ© **Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯** Ú©Ù‡ ØªÙˆØ³Ø· AI Ù‚Ø§Ø¨Ù„ Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ Ø§Ø³Øª.

### **ğŸ”„ Ø¬Ø±ÛŒØ§Ù† Ú©Ù„ÛŒ (High-Level Flow)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Submits   â”‚
â”‚   Website URL   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PHASE 1: WEB CRAWLING                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Crawl Job  â”‚â”€â”€â”€â–¶â”‚  Parse HTML  â”‚â”€â”€â”€â–¶â”‚  Clean Text  â”‚  â”‚
â”‚  â”‚   (Celery)   â”‚    â”‚  (BeautifulSoup)  â”‚  (Readability)   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PHASE 2: AI ANALYSIS                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Q&A Gen     â”‚    â”‚  Product     â”‚    â”‚  Summarize   â”‚  â”‚
â”‚  â”‚ (Gemini 2.5) â”‚    â”‚ Extraction   â”‚    â”‚   Content    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               PHASE 3: KNOWLEDGE STORAGE                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   QAPair     â”‚    â”‚   Product    â”‚    â”‚ WebsitePage  â”‚  â”‚
â”‚  â”‚   Model      â”‚    â”‚    Model     â”‚    â”‚    Model     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                    â”‚                    â”‚          â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                              â–¼                               â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                  â”‚  TenantKnowledge DB    â”‚                  â”‚
â”‚                  â”‚  (Chunked + Embedded)  â”‚                  â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           PHASE 4: SERVING (AI CHATBOT QUERY)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ User Query   â”‚â”€â”€â”€â–¶â”‚  Embedding   â”‚â”€â”€â”€â–¶â”‚   Vector     â”‚  â”‚
â”‚  â”‚              â”‚    â”‚  (OpenAI)    â”‚    â”‚   Search     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                   â”‚          â”‚
â”‚                                                   â–¼          â”‚
â”‚                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                                       â”‚  Gemini Response   â”‚ â”‚
â”‚                                       â”‚  (with context)    â”‚ â”‚
â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Ù…Ø¹Ù…Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„

### **ğŸ—ï¸ Component Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FRONTEND (React)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Website    â”‚  â”‚   Products   â”‚  â”‚      AI Chatbot      â”‚  â”‚
â”‚  â”‚   Manager    â”‚  â”‚    List      â”‚  â”‚   (Chat Interface)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                       â”‚
          â–¼                  â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DJANGO REST API                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  web_knowledgeâ”‚  â”‚  web_knowledgeâ”‚  â”‚     AI_model         â”‚  â”‚
â”‚  â”‚   /websites/  â”‚  â”‚   /products/ â”‚  â”‚   /ask-question/     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                       â”‚
          â–¼                  â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       BUSINESS LOGIC                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Celery Tasks (Async)                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ crawl_websiteâ”‚  â”‚ process_page â”‚  â”‚ generate_qa  â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      Services Layer                       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ProductExtractâ”‚  â”‚ContextRetrievâ”‚  â”‚GeminiService â”‚   â”‚   â”‚
â”‚  â”‚  â”‚     or       â”‚  â”‚      er      â”‚  â”‚              â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                                                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  Embedding   â”‚  â”‚SessionMemory â”‚  â”‚ QueryRouter  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚   Service    â”‚  â”‚   Manager    â”‚  â”‚              â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      EXTERNAL SERVICES                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Gemini AI  â”‚  â”‚  OpenAI API  â”‚  â”‚     Redis Cache      â”‚  â”‚
â”‚  â”‚  (Analysis)  â”‚  â”‚ (Embeddings) â”‚  â”‚                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATABASE LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  PostgreSQL  â”‚  â”‚  pgvector    â”‚  â”‚     Celery Beat      â”‚  â”‚
â”‚  â”‚   (Main DB)  â”‚  â”‚  (Vectors)   â”‚  â”‚   (Scheduling)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. ÙØ§Ø² 1: Web Crawling

### **ğŸ“ Ù…Ø³Ø¦ÙˆÙ„ÛŒØª**
Ø¯Ø±ÛŒØ§ÙØª URL Ø³Ø§ÛŒØª Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ…Ø§Ù… ØµÙØ­Ø§Øª Ùˆ Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù†â€ŒÙ‡Ø§.

### **ğŸ”§ Components**

#### **3.1. Models**

**ÙØ§ÛŒÙ„:** `src/web_knowledge/models.py`

```python
class WebsiteSource(models.Model):
    """Ø³Ø§ÛŒØª Ø§ØµÙ„ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ú©Ø±Ø§Ù„ Ø¨Ø´Ù‡"""
    id = models.UUIDField(primary_key=True, default=uuid.uuid4)
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    name = models.CharField(max_length=255)
    url = models.URLField()
    
    # Crawl settings
    max_pages = models.IntegerField(default=50)
    max_depth = models.IntegerField(default=3)
    respect_robots_txt = models.BooleanField(default=True)
    
    # AI settings
    auto_extract_products = models.BooleanField(default=True)
    
    created_at = models.DateTimeField(auto_now_add=True)


class WebsitePage(models.Model):
    """Ù‡Ø± ØµÙØ­Ù‡ Ú©Ø±Ø§Ù„ Ø´Ø¯Ù‡"""
    id = models.UUIDField(primary_key=True, default=uuid.uuid4)
    website = models.ForeignKey(WebsiteSource, related_name='pages')
    url = models.URLField(unique=True)
    
    # Content
    raw_html = models.TextField()  # HTML Ø®Ø§Ù…
    cleaned_content = models.TextField()  # Ù…ØªÙ† ØªÙ…ÛŒØ²
    summary = models.TextField(blank=True)
    
    # Metadata
    title = models.CharField(max_length=500)
    word_count = models.IntegerField(default=0)
    processing_status = models.CharField(
        max_length=20,
        choices=[
            ('pending', 'Pending'),
            ('processing', 'Processing'),
            ('completed', 'Completed'),
            ('failed', 'Failed')
        ],
        default='pending'
    )
    
    created_at = models.DateTimeField(auto_now_add=True)


class CrawlJob(models.Model):
    """ÙˆØ¶Ø¹ÛŒØª Ú©Ø±Ø§Ù„"""
    id = models.UUIDField(primary_key=True, default=uuid.uuid4)
    website = models.ForeignKey(WebsiteSource)
    status = models.CharField(
        max_length=20,
        choices=[
            ('pending', 'Pending'),
            ('running', 'Running'),
            ('completed', 'Completed'),
            ('failed', 'Failed')
        ]
    )
    
    # Progress tracking
    pages_discovered = models.IntegerField(default=0)
    pages_crawled = models.IntegerField(default=0)
    pages_failed = models.IntegerField(default=0)
    
    started_at = models.DateTimeField(auto_now_add=True)
    completed_at = models.DateTimeField(null=True)
```

---

#### **3.2. Crawling Service**

**ÙØ§ÛŒÙ„:** `src/web_knowledge/services/crawler_service.py`

```python
import requests
from bs4 import BeautifulSoup
from readability import Document
from urllib.parse import urljoin, urlparse
import logging

logger = logging.getLogger(__name__)


class WebCrawler:
    """
    Ú©Ø±Ø§Ù„Ø± ÙˆØ¨ - Ù…Ø³Ø¦ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ Ø§Ø² ØµÙØ­Ø§Øª
    """
    
    def __init__(self, website: WebsiteSource):
        self.website = website
        self.visited_urls = set()
        self.queue = [website.url]
        self.max_pages = website.max_pages
        self.max_depth = website.max_depth
    
    def crawl(self) -> dict:
        """
        Ø´Ø±ÙˆØ¹ Ú©Ø±Ø§Ù„
        
        Returns:
            {
                'pages_crawled': 10,
                'pages_failed': 1,
                'pages': [WebsitePage, ...]
            }
        """
        pages_created = []
        failed_count = 0
        
        while self.queue and len(pages_created) < self.max_pages:
            url = self.queue.pop(0)
            
            if url in self.visited_urls:
                continue
            
            try:
                page = self._crawl_page(url)
                if page:
                    pages_created.append(page)
                    
                    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
                    new_links = self._extract_links(page.raw_html, url)
                    self.queue.extend(new_links)
                
                self.visited_urls.add(url)
                
            except Exception as e:
                logger.error(f"Failed to crawl {url}: {e}")
                failed_count += 1
        
        return {
            'pages_crawled': len(pages_created),
            'pages_failed': failed_count,
            'pages': pages_created
        }
    
    def _crawl_page(self, url: str) -> WebsitePage:
        """
        Ú©Ø±Ø§Ù„ ÛŒÚ© ØµÙØ­Ù‡
        """
        # Ø¯Ø±ÛŒØ§ÙØª HTML
        response = requests.get(url, timeout=30, headers={
            'User-Agent': 'Mozilla/5.0 (compatible; FikoBot/1.0)'
        })
        response.raise_for_status()
        
        html = response.text
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø§ Readability
        doc = Document(html)
        title = doc.title()
        cleaned_html = doc.summary()
        
        # ØªØ¨Ø¯ÛŒÙ„ HTML Ø¨Ù‡ Ù…ØªÙ†
        soup = BeautifulSoup(cleaned_html, 'html.parser')
        cleaned_content = soup.get_text(separator='\n', strip=True)
        
        # Ø­Ø°Ù Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ
        cleaned_content = '\n'.join([
            line for line in cleaned_content.split('\n')
            if line.strip()
        ])
        
        # Ø³Ø§Ø®Øª WebsitePage
        page = WebsitePage.objects.create(
            website=self.website,
            url=url,
            title=title,
            raw_html=html[:50000],  # Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ 50KB
            cleaned_content=cleaned_content,
            word_count=len(cleaned_content.split()),
            processing_status='pending'
        )
        
        logger.info(f"âœ… Crawled: {url} ({page.word_count} words)")
        
        return page
    
    def _extract_links(self, html: str, base_url: str) -> list:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ ØµÙØ­Ù‡
        """
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ URL Ú©Ø§Ù…Ù„
            full_url = urljoin(base_url, href)
            
            # ÙÙ‚Ø· Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ù‡Ù…ÛŒÙ† Ø¯Ø§Ù…Ù†Ù‡
            if self._is_same_domain(full_url, self.website.url):
                # Ø­Ø°Ù fragment (#section)
                full_url = full_url.split('#')[0]
                
                if full_url not in self.visited_urls:
                    links.append(full_url)
        
        return list(set(links))  # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
    
    def _is_same_domain(self, url1: str, url2: str) -> bool:
        """Ú†Ú© Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ú©Ù‡ Ø¯Ùˆ URL Ø§Ø² ÛŒÚ© Ø¯Ø§Ù…Ù†Ù‡ Ù‡Ø³ØªÙ†"""
        domain1 = urlparse(url1).netloc
        domain2 = urlparse(url2).netloc
        return domain1 == domain2
```

---

#### **3.3. Celery Task**

**ÙØ§ÛŒÙ„:** `src/web_knowledge/tasks.py`

```python
from celery import shared_task
from web_knowledge.models import WebsiteSource, CrawlJob
from web_knowledge.services.crawler_service import WebCrawler


@shared_task(bind=True, max_retries=3)
def crawl_website_task(self, website_id: str, crawl_job_id: str):
    """
    ØªØ³Ú© Celery Ø¨Ø±Ø§ÛŒ Ú©Ø±Ø§Ù„ ÙˆØ¨Ø³Ø§ÛŒØª (async)
    
    Ø§ÛŒÙ† ØªØ³Ú© background Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´Ù‡ Ùˆ Ø³Ù†Ú¯ÛŒÙ†â€ŒØªØ±ÛŒÙ† Ø¨Ø®Ø´ Ø³ÛŒØ³ØªÙ…Ù‡
    """
    try:
        website = WebsiteSource.objects.get(id=website_id)
        crawl_job = CrawlJob.objects.get(id=crawl_job_id)
        
        # Ø´Ø±ÙˆØ¹ Ú©Ø±Ø§Ù„
        crawl_job.status = 'running'
        crawl_job.save()
        
        crawler = WebCrawler(website)
        result = crawler.crawl()
        
        # Ø¢Ù¾Ø¯ÛŒØª ÙˆØ¶Ø¹ÛŒØª
        crawl_job.status = 'completed'
        crawl_job.pages_discovered = len(crawler.visited_urls)
        crawl_job.pages_crawled = result['pages_crawled']
        crawl_job.pages_failed = result['pages_failed']
        crawl_job.completed_at = timezone.now()
        crawl_job.save()
        
        # ÙØ±Ø³ØªØ§Ø¯Ù† ØµÙØ­Ø§Øª Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ AI
        for page in result['pages']:
            process_page_content_task.delay(str(page.id))
        
        logger.info(
            f"âœ… Crawl completed for {website.name}: "
            f"{result['pages_crawled']} pages"
        )
        
        return {
            'success': True,
            'pages_crawled': result['pages_crawled']
        }
        
    except Exception as e:
        logger.error(f"âŒ Crawl failed: {e}")
        
        # Retry Ø¨Ø§ exponential backoff
        raise self.retry(exc=e, countdown=60 * (2 ** self.request.retries))
```

---

## 4. ÙØ§Ø² 2: AI Analysis

### **ğŸ“ Ù…Ø³Ø¦ÙˆÙ„ÛŒØª**
ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø±Ø§Ù„ Ø´Ø¯Ù‡ Ø¨Ø§ AI:
1. ØªÙˆÙ„ÛŒØ¯ Q&A
2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØµÙˆÙ„Ø§Øª
3. Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ

### **ğŸ”§ Components**

#### **4.1. Q&A Generation**

**ÙØ§ÛŒÙ„:** `src/web_knowledge/tasks.py`

```python
@shared_task(bind=True)
def process_page_content_task(self, page_id: str):
    """
    Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø­ØªÙˆØ§ÛŒ ÛŒÚ© ØµÙØ­Ù‡ Ø¨Ø§ AI
    
    Ø´Ø§Ù…Ù„:
    1. ØªÙˆÙ„ÛŒØ¯ Q&A pairs
    2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØµÙˆÙ„Ø§Øª (Ø§Ú¯Ù‡ ÙØ¹Ø§Ù„ Ø¨Ø§Ø´Ù‡)
    3. Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ
    """
    try:
        page = WebsitePage.objects.get(id=page_id)
        
        # ØªØºÛŒÛŒØ± ÙˆØ¶Ø¹ÛŒØª Ø¨Ù‡ processing
        page.processing_status = 'processing'
        page.save()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ø¨Ø®Ø´ 1: ØªÙˆÙ„ÛŒØ¯ Q&A Ø¨Ø§ Gemini
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        qa_pairs = generate_qa_from_content(page)
        
        if qa_pairs:
            logger.info(f"âœ… Generated {len(qa_pairs)} Q&A pairs for {page.url}")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ø¨Ø®Ø´ 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØµÙˆÙ„Ø§Øª (Ø§Ú¯Ù‡ ÙØ¹Ø§Ù„ Ø¨Ø§Ø´Ù‡)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        if page.website.auto_extract_products:
            try:
                from web_knowledge.services.product_extractor import ProductExtractor
                
                extractor = ProductExtractor(page.website.user)
                products = extractor.extract_and_save(page)
                
                if products:
                    logger.info(
                        f"âœ… Extracted {len(products)} products from {page.url}"
                    )
            except Exception as e:
                logger.error(f"Product extraction failed: {e}")
                # Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯ÛŒÙ… Ú†ÙˆÙ† Q&A Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯Ù‡
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ø¨Ø®Ø´ 3: Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­ØªÙˆØ§
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        summary = summarize_page_content(page)
        if summary:
            page.summary = summary
        
        # ØªØºÛŒÛŒØ± ÙˆØ¶Ø¹ÛŒØª Ø¨Ù‡ completed
        page.processing_status = 'completed'
        page.save()
        
        logger.info(f"âœ… Page processing completed: {page.url}")
        
        return {
            'success': True,
            'qa_pairs': len(qa_pairs),
            'products': len(products) if 'products' in locals() else 0
        }
        
    except Exception as e:
        logger.error(f"âŒ Page processing failed for {page_id}: {e}")
        
        page.processing_status = 'failed'
        page.save()
        
        raise


def generate_qa_from_content(page: WebsitePage) -> list:
    """
    ØªÙˆÙ„ÛŒØ¯ Q&A pairs Ø¨Ø§ Gemini 2.5 Flash
    
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ø² Gemini Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ ØªØ§ Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨
    Ù…ØªØ¯Ø§ÙˆÙ„ Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ù‡ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ù‡
    """
    import google.generativeai as genai
    from settings.models import GeneralSettings
    
    # Configuration
    api_key = GeneralSettings.get_settings().gemini_api_key
    genai.configure(api_key=api_key)
    
    # Safety settings (Ø¨Ø±Ø§ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§Ø±Ø³ÛŒ)
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"},
    ]
    
    model = genai.GenerativeModel(
        'gemini-2.5-flash',  # Ø³Ø±ÛŒØ¹ Ùˆ Ø§Ø±Ø²Ø§Ù† Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Q&A
        safety_settings=safety_settings
    )
    
    # Prompt Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Q&A
    prompt = f"""Based on the following content, generate 5-10 frequently asked questions (FAQ) and their answers.

RULES:
- Questions should be natural and commonly asked
- Answers should be accurate based ONLY on the provided content
- Return JSON format: {{"qa_pairs": [{{"question": "...", "answer": "..."}}, ...]}}
- If content doesn't have enough information for 5 Q&As, generate fewer

CONTENT:
Title: {page.title}
URL: {page.url}

{page.cleaned_content[:4000]}

Return ONLY valid JSON:"""
    
    try:
        response = model.generate_content(
            prompt,
            generation_config={
                'temperature': 0.3,
                'max_output_tokens': 2000,
            },
            safety_settings=safety_settings
        )
        
        # Parse JSON
        import json
        import re
        
        response_text = response.text.strip()
        
        # Ø­Ø°Ù markdown code blocks
        if '```' in response_text:
            response_text = re.sub(r'```json\n?|\n?```', '', response_text).strip()
        
        result = json.loads(response_text)
        
        # Ø°Ø®ÛŒØ±Ù‡ Q&A pairs
        qa_pairs = []
        for qa in result.get('qa_pairs', []):
            qa_pair = QAPair.objects.create(
                user=page.website.user,
                page=page,
                question=qa['question'],
                answer=qa['answer'],
                source_type='website',
                is_active=True
            )
            qa_pairs.append(qa_pair)
        
        return qa_pairs
        
    except Exception as e:
        logger.error(f"Q&A generation failed for {page.url}: {e}")
        return []
```

---

#### **4.2. Product Extraction**

**ÙØ§ÛŒÙ„:** `src/web_knowledge/services/product_extractor.py`

```python
class ProductExtractor:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¨Ø§ AI
    
    Ø§Ø² Gemini 2.5 Pro Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§
    """
    
    def __init__(self, user):
        self.user = user
        self.gemini_model = self._init_gemini()
    
    def _init_gemini(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Gemini 2.5 Pro"""
        import google.generativeai as genai
        from settings.models import GeneralSettings
        
        api_key = GeneralSettings.get_settings().gemini_api_key
        genai.configure(api_key=api_key)
        
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"},
        ]
        
        model = genai.GenerativeModel(
            'gemini-2.5-pro',  # Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø¯Ù‚Øª Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØµÙˆÙ„
            safety_settings=safety_settings
        )
        
        return model
    
    def extract_products_ai(self, page: WebsitePage) -> list:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¨Ø§ Gemini 2.5 Pro
        
        Returns:
            [
                {
                    'title': 'Ù†Ø§Ù… Ù…Ø­ØµÙˆÙ„',
                    'price': 150000,
                    'currency': 'IRT',
                    'description': '...',
                    'brand': 'Ø¨Ø±Ù†Ø¯',
                    'features': ['ÙˆÛŒÚ˜Ú¯ÛŒ 1', 'ÙˆÛŒÚ˜Ú¯ÛŒ 2'],
                    ...
                },
                ...
            ]
        """
        # Prompt Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØµÙˆÙ„
        prompt = f"""Extract product/service information from this page.

PAGE CONTENT:
Title: {page.title}
URL: {page.url}

{page.cleaned_content[:4000]}

EXTRACT:
1. title: Product name
2. price: Numeric price only (e.g., 150000 not "$150,000")
3. currency: USD, EUR, IRT, etc.
4. description: Detailed description
5. brand: Brand name
6. category: Product category
7. features: Array of key features
8. in_stock: Boolean

Return JSON:
{{
  "has_products": true,
  "products": [
    {{
      "title": "...",
      "price": 150000,
      "currency": "IRT",
      "description": "...",
      "brand": "...",
      "category": "...",
      "features": ["..."],
      "in_stock": true
    }}
  ]
}}

If NO products found, return: {{"has_products": false, "products": []}}

Return ONLY valid JSON:"""
        
        try:
            response = self.gemini_model.generate_content(
                prompt,
                generation_config={
                    'temperature': 0.1,  # Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§
                    'max_output_tokens': 3000,
                },
                safety_settings=[
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"},
                ]
            )
            
            # Parse JSON
            import json
            import re
            
            response_text = response.text.strip()
            
            if '```' in response_text:
                response_text = re.sub(r'```json\n?|\n?```', '', response_text).strip()
            
            result = json.loads(response_text)
            
            if result.get('has_products') and result.get('products'):
                logger.info(
                    f"âœ… Gemini 2.5 Pro extracted {len(result['products'])} "
                    f"products from {page.url}"
                )
                return result['products']
            
            return []
            
        except Exception as e:
            logger.error(f"AI extraction failed for {page.url}: {e}")
            return []
    
    def save_products(self, products_data: list, source_page, source_website) -> list:
        """
        Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        
        Ø¨Ø¹Ø¯ Ø§Ø² saveØŒ signal Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø­ØµÙˆÙ„ Ø±Ùˆ Ø¨Ù‡
        TenantKnowledge Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ (Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ AI)
        """
        from web_knowledge.models import Product
        from decimal import Decimal
        
        saved_products = []
        
        for data in products_data:
            try:
                # Ú†Ú© Ú©Ø±Ø¯Ù† ØªÚ©Ø±Ø§Ø±ÛŒ
                existing = Product.objects.filter(
                    user=self.user,
                    title__iexact=data.get('title', '').strip()
                ).first()
                
                if existing:
                    logger.info(f"â­ï¸ Product already exists: {data.get('title')}")
                    continue
                
                # Ø³Ø§Ø®Øª Product
                product = Product.objects.create(
                    user=self.user,
                    title=data.get('title', 'Untitled').strip(),
                    description=data.get('description', ''),
                    price=Decimal(str(data.get('price'))) if data.get('price') else None,
                    currency=data.get('currency', 'USD'),
                    brand=data.get('brand', ''),
                    category=data.get('category', ''),
                    features=data.get('features', []),
                    in_stock=data.get('in_stock', True),
                    link=source_page.url,
                    source_website=source_website,
                    source_page=source_page,
                    extraction_method='ai_auto',
                    extraction_confidence=0.95,
                    is_active=True
                )
                
                saved_products.append(product)
                
                logger.info(
                    f"âœ… Saved product: {product.title} "
                    f"({product.price} {product.currency})"
                )
                
                # ğŸ¯ IMPORTANT: Signal Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø­ØµÙˆÙ„ Ø±Ùˆ Ø¨Ù‡
                # TenantKnowledge Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ (ÙØ§ÛŒÙ„ signals.py)
                # Ø¯ÛŒÚ¯Ù‡ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ manual chunking Ù†ÛŒØ³Øª!
                
            except Exception as e:
                logger.error(f"Failed to save product: {e}")
                continue
        
        return saved_products
```

---

## 5. ÙØ§Ø² 3: Knowledge Storage

### **ğŸ“ Ù…Ø³Ø¦ÙˆÙ„ÛŒØª**
Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ù†Ø´ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø¯Ø± ÙØ±Ù…ØªÛŒ Ú©Ù‡ AI Ø¨ØªÙˆÙ†Ù‡ Ø¬Ø³ØªØ¬Ùˆ Ú©Ù†Ù‡.

### **ğŸ”§ TenantKnowledge Model**

**ÙØ§ÛŒÙ„:** `src/AI_model/models.py`

```python
class TenantKnowledge(models.Model):
    """
    Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ù‡Ø± Ú©Ø§Ø±Ø¨Ø± (Ø¨Ø§ pgvector)
    
    Ø§ÛŒÙ† model Ù‡Ù…Ù‡ Ø¯Ø§Ù†Ø´ user Ø±Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡:
    - Q&A pairs
    - Products
    - Website content
    - Manual prompts
    
    Ø¨Ø§ embedding Ø¨Ø±Ø§ÛŒ semantic search
    """
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4)
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    
    # Ù†ÙˆØ¹ Ù…Ø­ØªÙˆØ§
    chunk_type = models.CharField(
        max_length=20,
        choices=[
            ('faq', 'FAQ'),
            ('product', 'Product'),
            ('website', 'Website Content'),
            ('manual', 'Manual Prompt')
        ]
    )
    
    # Ø´Ù†Ø§Ø³Ù‡ Ù…Ù†Ø¨Ø¹ (Ù…Ø«Ù„Ø§Ù‹ product_id ÛŒØ§ qa_pair_id)
    source_id = models.UUIDField(null=True)
    document_id = models.UUIDField(null=True)  # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ chunks Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù‡Ù…
    
    # Ù…Ø­ØªÙˆØ§
    section_title = models.CharField(max_length=500)
    full_text = models.TextField()  # Ù…ØªÙ† Ú©Ø§Ù…Ù„
    tldr = models.TextField()  # Ø®Ù„Ø§ØµÙ‡ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø±ÛŒØ¹
    
    # Embeddings (Ø¨Ø±Ø§ÛŒ semantic search)
    tldr_embedding = VectorField(dimensions=1536, null=True)  # OpenAI embedding
    full_embedding = VectorField(dimensions=1536, null=True)
    
    # Metadata
    language = models.CharField(max_length=10, default='en')
    word_count = models.IntegerField(default=0)
    metadata = models.JSONField(default=dict, blank=True)
    
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        indexes = [
            models.Index(fields=['user', 'chunk_type']),
            models.Index(fields=['source_id']),
        ]
        # Index Ø¨Ø±Ø§ÛŒ pgvector
        # CREATE INDEX ON tenant_knowledge USING ivfflat (tldr_embedding vector_cosine_ops);
```

---

### **ğŸ”§ Auto-Sync Ø¨Ø§ Signals**

**ÙØ§ÛŒÙ„:** `src/web_knowledge/signals.py`

```python
from django.db.models.signals import post_save, post_delete
from django.dispatch import receiver


@receiver(post_save, sender='web_knowledge.Product')
def sync_product_to_knowledge_base(sender, instance, created, **kwargs):
    """
    Ù‡Ø± ÙˆÙ‚Øª Product Ø³Ø§Ø®ØªÙ‡ ÛŒØ§ update Ù…ÛŒâ€ŒØ´Ù‡ØŒ
    Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ù‡ TenantKnowledge Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´Ù‡
    
    Ø§ÛŒÙ† ÛŒØ¹Ù†ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª AUTOMATIC ØªÙˆØ³Ø· AI Ù‚Ø§Ø¨Ù„ Ø¬Ø³ØªØ¬Ùˆ Ù‡Ø³ØªÙ†!
    """
    try:
        from AI_model.models import TenantKnowledge
        from AI_model.services.embedding_service import EmbeddingService
        
        # Ø§Ú¯Ù‡ inactive Ø´Ø¯ØŒ Ù¾Ø§Ú©Ø´ Ú©Ù†
        if not instance.is_active:
            TenantKnowledge.objects.filter(
                user=instance.user,
                chunk_type='product',
                source_id=instance.id
            ).delete()
            logger.info(f"ğŸ—‘ï¸ Removed inactive product: {instance.title}")
            return
        
        # Ø³Ø§Ø®Øª Ù…ØªÙ† Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ embedding
        full_text = f"Product: {instance.title}\n"
        full_text += f"Type: {instance.get_product_type_display()}\n"
        
        if instance.description:
            full_text += f"Description: {instance.description}\n"
        
        if instance.price:
            full_text += f"Price: {instance.price} {instance.currency}\n"
            
            if instance.original_price and instance.original_price > instance.price:
                discount = ((instance.original_price - instance.price) / instance.original_price) * 100
                full_text += f"Discount: {discount:.0f}% OFF\n"
        
        if instance.features:
            full_text += f"Features: {', '.join(instance.features[:5])}\n"
        
        if instance.brand:
            full_text += f"Brand: {instance.brand}\n"
        
        if instance.link:
            full_text += f"Link: {instance.link}\n"
        
        # ØªÙˆÙ„ÛŒØ¯ embeddings Ø¨Ø§ OpenAI
        embedding_service = EmbeddingService()
        
        # Ø®Ù„Ø§ØµÙ‡ Ø¨Ø±Ø§ÛŒ TL;DR
        tldr = f"{instance.title} - {instance.price} {instance.currency}"
        if instance.short_description:
            tldr += f" - {instance.short_description[:100]}"
        
        tldr_embedding = embedding_service.get_embedding(tldr)
        full_embedding = embedding_service.get_embedding(full_text)
        
        if not tldr_embedding or not full_embedding:
            logger.warning(f"Failed to generate embeddings for: {instance.title}")
            return
        
        # Ø³Ø§Ø®Øª ÛŒØ§ Ø¢Ù¾Ø¯ÛŒØª chunk
        chunk, chunk_created = TenantKnowledge.objects.update_or_create(
            user=instance.user,
            chunk_type='product',
            source_id=instance.id,
            defaults={
                'section_title': instance.title,
                'full_text': full_text,
                'tldr': tldr,
                'tldr_embedding': tldr_embedding,
                'full_embedding': full_embedding,
                'language': 'fa' if _is_persian(instance.title) else 'en',
                'word_count': len(full_text.split()),
                'metadata': {
                    'price': float(instance.price) if instance.price else None,
                    'currency': instance.currency,
                    'brand': instance.brand or '',
                    'link': instance.link or '',
                    'extraction_method': instance.extraction_method,
                }
            }
        )
        
        action = "Added" if chunk_created else "Updated"
        logger.info(f"âœ… {action} product in knowledge base: {instance.title}")
        
    except Exception as e:
        logger.error(f"Failed to sync product to knowledge base: {e}")


def _is_persian(text: str) -> bool:
    """ØªØ´Ø®ÛŒØµ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ"""
    return any('\u0600' <= c <= '\u06FF' for c in text)
```

---

## 6. ÙØ§Ø² 4: Serving with OpenAI

### **ğŸ“ Ù…Ø³Ø¦ÙˆÙ„ÛŒØª**
ÙˆÙ‚ØªÛŒ user Ø³ÙˆØ§Ù„ Ù…ÛŒâ€ŒÙ¾Ø±Ø³Ù‡ØŒ Ø³ÛŒØ³ØªÙ…:
1. Ø³ÙˆØ§Ù„ Ø±Ùˆ Ø¨Ø§ **OpenAI embed** Ù…ÛŒâ€ŒÚ©Ù†Ù‡
2. Ø¨Ø§ **pgvector** Ù…Ø±ØªØ¨Ø·â€ŒØªØ±ÛŒÙ† chunks Ø±Ùˆ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
3. Ø¨Ù‡ **Gemini** Ù…ÛŒâ€ŒÙØ±Ø³ØªÙ‡ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®

### **ğŸ”§ Components**

#### **6.1. Embedding Service**

**ÙØ§ÛŒÙ„:** `src/AI_model/services/embedding_service.py`

```python
class EmbeddingService:
    """
    Ø³Ø±ÙˆÛŒØ³ ØªÙˆÙ„ÛŒØ¯ embedding Ø¨Ø§ OpenAI
    
    Ø§Ø² OpenAI text-embedding-3-large Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
    Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ vector
    """
    
    def __init__(self):
        self.openai_client = self._init_openai()
        self.gemini_model = self._init_gemini_fallback()
    
    def _init_openai(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ OpenAI Ø¨Ø±Ø§ÛŒ embedding"""
        from openai import OpenAI
        from settings.models import GeneralSettings
        
        api_key = GeneralSettings.get_settings().openai_api_key
        
        if not api_key:
            logger.warning("OpenAI API key not configured")
            return None
        
        client = OpenAI(api_key=api_key)
        logger.info("âœ… OpenAI embedding initialized")
        
        return client
    
    def get_embedding(self, text: str) -> list:
        """
        ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ embedding vector
        
        Args:
            text: Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ
        
        Returns:
            Vector Ø¨Ø§ 1536 dimension (OpenAI text-embedding-3-large)
        """
        try:
            # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ Ù…ØªÙ† (max 8191 tokens)
            text = text[:30000]  # ~8000 tokens
            
            # ØªÙˆÙ„ÛŒØ¯ embedding Ø¨Ø§ OpenAI
            response = self.openai_client.embeddings.create(
                model="text-embedding-3-large",  # Ø¨Ù‡ØªØ±ÛŒÙ† model OpenAI
                input=text,
                encoding_format="float"
            )
            
            embedding = response.data[0].embedding
            
            logger.debug(f"âœ… Generated embedding: {len(embedding)} dimensions")
            
            return embedding
            
        except Exception as e:
            logger.error(f"OpenAI embedding failed: {e}")
            
            # Fallback Ø¨Ù‡ Gemini
            return self._get_gemini_embedding(text)
    
    def _get_gemini_embedding(self, text: str) -> list:
        """Fallback: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Gemini Ø¨Ø±Ø§ÛŒ embedding"""
        try:
            import google.generativeai as genai
            
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=text[:20000],
                task_type="retrieval_document"
            )
            
            # Gemini embedding 768 dimension Ø¯Ø§Ø±Ù‡ØŒ ÙˆÙ„ÛŒ Ø¨Ø§ÛŒØ¯ 1536 Ø¨Ø´Ù‡
            # Ù¾Ø³ padding Ù…ÛŒâ€ŒØ²Ù†ÛŒÙ…
            embedding = result['embedding']
            embedding = embedding + [0.0] * (1536 - len(embedding))
            
            return embedding
            
        except Exception as e:
            logger.error(f"Gemini embedding also failed: {e}")
            return None
```

---

#### **6.2. Context Retriever**

**ÙØ§ÛŒÙ„:** `src/AI_model/services/context_retriever.py`

```python
class ContextRetriever:
    """
    Retrieval Augmented Generation (RAG) Ø¨Ø§ pgvector
    
    Ø¬Ø³ØªØ¬ÙˆÛŒ semantic Ø¨Ø§ cosine similarity
    """
    
    @classmethod
    def retrieve_context(
        cls,
        query: str,
        user,
        primary_source: str,  # 'products', 'faq', 'website', etc.
        secondary_sources: list,
        primary_budget: int,
        secondary_budget: int,
        routing_info: dict = None
    ) -> dict:
        """
        Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø±ØªØ¨Ø·â€ŒØªØ±ÛŒÙ† context Ø¨Ø±Ø§ÛŒ query
        
        Args:
            query: Ø³ÙˆØ§Ù„ user
            user: Ú©Ø§Ø±Ø¨Ø±
            primary_source: Ù…Ù†Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¬Ø³ØªØ¬Ùˆ
            secondary_sources: Ù…Ù†Ø§Ø¨Ø¹ ÙØ±Ø¹ÛŒ
            primary_budget: Ø­Ø¯Ø§Ú©Ø«Ø± token Ø¨Ø±Ø§ÛŒ primary
            secondary_budget: Ø­Ø¯Ø§Ú©Ø«Ø± token Ø¨Ø±Ø§ÛŒ secondary
        
        Returns:
            {
                'primary_context': [
                    {
                        'title': 'Ù…Ø­ØµÙˆÙ„ X',
                        'content': 'ØªÙˆØ¶ÛŒØ­Ø§Øª...',
                        'score': 0.95,
                        'type': 'product'
                    },
                    ...
                ],
                'secondary_context': [...],
                'metadata': {
                    'primary_source': 'products',
                    'avg_similarity': 0.85
                }
            }
        """
        from AI_model.services.embedding_service import EmbeddingService
        
        # 1. ØªØ¨Ø¯ÛŒÙ„ query Ø¨Ù‡ embedding (Ø¨Ø§ OpenAI)
        embedding_service = EmbeddingService()
        query_embedding = embedding_service.get_embedding(query)
        
        if not query_embedding:
            logger.error("Failed to generate query embedding")
            return {
                'primary_context': [],
                'secondary_context': [],
                'metadata': {'error': 'embedding_failed'}
            }
        
        # 2. Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± primary source
        primary_results = cls._search_source(
            user=user,
            source=primary_source,
            query_embedding=query_embedding,
            top_k=5,
            token_budget=primary_budget
        )
        
        # 3. Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± secondary sources
        secondary_results = []
        for source in secondary_sources[:2]:  # max 2 secondary
            results = cls._search_source(
                user=user,
                source=source,
                query_embedding=query_embedding,
                top_k=3,
                token_budget=secondary_budget
            )
            secondary_results.extend(results)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† similarity
        all_scores = [r['score'] for r in primary_results + secondary_results]
        avg_similarity = sum(all_scores) / len(all_scores) if all_scores else 0
        
        return {
            'primary_context': primary_results,
            'secondary_context': secondary_results,
            'metadata': {
                'primary_source': primary_source,
                'avg_similarity': round(avg_similarity, 3),
                'total_results': len(primary_results) + len(secondary_results)
            }
        }
    
    @classmethod
    def _search_source(
        cls,
        user,
        source: str,
        query_embedding: list,
        top_k: int,
        token_budget: int
    ) -> list:
        """
        Ø¬Ø³ØªØ¬ÙˆÛŒ semantic Ø¯Ø± ÛŒÚ© source Ø¨Ø§ pgvector
        
        Ø§Ø² cosine similarity Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
        """
        try:
            from AI_model.models import TenantKnowledge, PGVECTOR_AVAILABLE
            from pgvector.django import CosineDistance
            
            # ØªØ¨Ø¯ÛŒÙ„ source Ø¨Ù‡ chunk_type
            SOURCE_TO_CHUNK_TYPE = {
                'faq': 'faq',
                'products': 'product',
                'website': 'website',
                'manual': 'manual'
            }
            
            chunk_type = SOURCE_TO_CHUNK_TYPE.get(source, source)
            
            # Query Ø¨Ø§ pgvector
            chunks = TenantKnowledge.objects.filter(
                user=user,
                chunk_type=chunk_type,
                tldr_embedding__isnull=False
            ).annotate(
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ cosine distance
                distance=CosineDistance('tldr_embedding', query_embedding)
            ).order_by('distance')[:top_k * 2]  # Ú¯Ø±ÙØªÙ† 2x Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ±
            
            results = []
            total_tokens = 0
            
            for chunk in chunks:
                similarity = 1 - chunk.distance  # ØªØ¨Ø¯ÛŒÙ„ distance Ø¨Ù‡ similarity
                
                # ÙÛŒÙ„ØªØ±: ÙÙ‚Ø· Ù†ØªØ§ÛŒØ¬ Ø¨Ø§ similarity > 0.1
                if similarity < 0.1:
                    continue
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ tokens ØªÙ‚Ø±ÛŒØ¨ÛŒ
                chunk_tokens = chunk.word_count * 1.3
                
                if total_tokens + chunk_tokens > token_budget:
                    break  # Ø¨ÙˆØ¯Ø¬Ù‡ ØªÙ…ÙˆÙ… Ø´Ø¯
                
                results.append({
                    'title': chunk.section_title,
                    'content': chunk.full_text,
                    'type': chunk.chunk_type,
                    'score': round(similarity, 3),
                    'source_id': chunk.source_id,
                    'word_count': chunk.word_count,
                    'metadata': chunk.metadata
                })
                
                total_tokens += chunk_tokens
                
                if len(results) >= top_k:
                    break
            
            logger.info(
                f"ğŸ” Found {len(results)} results from {source} "
                f"(avg similarity: {sum(r['score'] for r in results) / len(results) if results else 0:.2f})"
            )
            
            return results
            
        except Exception as e:
            logger.error(f"Search failed for source {source}: {e}")
            return []
```

---

## 7. Q&A System

### **ğŸ“ Model**

**ÙØ§ÛŒÙ„:** `src/web_knowledge/models.py`

```python
class QAPair(models.Model):
    """
    Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨ Ù…ØªØ¯Ø§ÙˆÙ„
    
    Ù…ÛŒâ€ŒØªÙˆÙ†Ù‡ Ø§Ø² Ú†Ù†Ø¯ Ù…Ù†Ø¨Ø¹ Ø¨ÛŒØ§Ø¯:
    1. Auto-generated (AI Ø§Ø² ØµÙØ­Ø§Øª website)
    2. Manual (user Ø®ÙˆØ¯Ø´ Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³Ù‡)
    """
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4)
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    
    # Source
    source_type = models.CharField(
        max_length=20,
        choices=[
            ('website', 'Website'),
            ('manual', 'Manual Entry')
        ],
        default='manual'
    )
    page = models.ForeignKey(
        WebsitePage,
        null=True,
        blank=True,
        on_delete=models.SET_NULL,
        help_text="ØµÙØ­Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ø§ÛŒÙ† Q&A Ø§Ø² Ø§ÙˆÙ† generate Ø´Ø¯Ù‡"
    )
    
    # Content
    question = models.TextField()
    answer = models.TextField()
    
    # Metadata
    category = models.CharField(max_length=100, blank=True)
    tags = ArrayField(models.CharField(max_length=50), default=list, blank=True)
    
    is_active = models.BooleanField(default=True)
    
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['user', 'is_active']),
            models.Index(fields=['source_type']),
        ]
    
    def __str__(self):
        return f"Q: {self.question[:50]}"
```

### **ğŸ“ Auto-Ingestion Ø¨Ù‡ TenantKnowledge**

Q&A pairs Ù‡Ù… Ù…Ø«Ù„ Products Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ù‡ TenantKnowledge Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´Ù†:

**ÙØ§ÛŒÙ„:** `src/AI_model/services/knowledge_ingestion_service.py`

```python
@classmethod
def _ingest_faq(cls, user) -> int:
    """
    Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Q&A pairs Ø¨Ù‡ TenantKnowledge
    
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ ØªÙˆØ³Ø· management command Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´Ù‡
    ÛŒØ§ manual ØªÙˆØ³Ø· user trigger Ù…ÛŒâ€ŒØ´Ù‡
    """
    try:
        from web_knowledge.models import QAPair
        from AI_model.models import TenantKnowledge
        from AI_model.services.embedding_service import EmbeddingService
        
        # Ú¯Ø±ÙØªÙ† Ù‡Ù…Ù‡ Q&A Ù‡Ø§ÛŒ ÙØ¹Ø§Ù„
        qa_pairs = QAPair.objects.filter(user=user, is_active=True)
        
        chunks_created = 0
        embedding_service = EmbeddingService()
        
        for qa in qa_pairs:
            # Ø³Ø§Ø®Øª Ù…ØªÙ† Ú©Ø§Ù…Ù„
            full_text = f"Q: {qa.question}\nA: {qa.answer}"
            
            # TL;DR (Ø®Ù„Ø§ØµÙ‡): ÙÙ‚Ø· Ø³ÙˆØ§Ù„
            tldr = qa.question
            
            # ØªÙˆÙ„ÛŒØ¯ embeddings
            tldr_embedding = embedding_service.get_embedding(tldr)
            full_embedding = embedding_service.get_embedding(full_text)
            
            if not tldr_embedding or not full_embedding:
                logger.warning(f"Failed to generate embeddings for Q&A {qa.id}")
                continue
            
            # Ø³Ø§Ø®Øª chunk
            TenantKnowledge.objects.update_or_create(
                user=user,
                chunk_type='faq',
                source_id=qa.id,
                defaults={
                    'section_title': qa.question[:200],
                    'full_text': full_text,
                    'tldr': tldr,
                    'tldr_embedding': tldr_embedding,
                    'full_embedding': full_embedding,
                    'language': _detect_language(qa.question),
                    'word_count': len(full_text.split()),
                    'metadata': {
                        'category': qa.category or '',
                        'tags': qa.tags or [],
                        'source_type': qa.source_type
                    }
                }
            )
            chunks_created += 1
        
        return chunks_created
        
    except Exception as e:
        logger.error(f"FAQ ingestion failed: {e}")
        raise
```

---

## 8. Products System

### **ğŸ“ Complete Product Model**

**ÙØ§ÛŒÙ„:** `src/web_knowledge/models.py`

```python
class Product(models.Model):
    """
    Ù…Ø­ØµÙˆÙ„Ø§Øª Ùˆ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§
    
    ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ:
    - Auto-extraction Ø¨Ø§ AI
    - Auto-sync Ø¨Ù‡ TenantKnowledge (Ø¨Ø§ signal)
    - Support Ø¨Ø±Ø§ÛŒ Ù‚ÛŒÙ…Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡ (ØªØ®ÙÛŒÙØŒ Ø¯ÙˆØ±Ù‡ Ù¾Ø±Ø¯Ø§Ø®ØªØŒ etc.)
    """
    
    id = models.UUIDField(primary_key=True, default=uuid.uuid4)
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    
    # â•â•â•â•â•â• Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ØµÙ„ÛŒ â•â•â•â•â•â•
    title = models.CharField(max_length=255)
    product_type = models.CharField(
        max_length=20,
        choices=[
            ('product', 'Product'),
            ('service', 'Service'),
            ('software', 'Software'),
            ('course', 'Course'),
            ('tool', 'Tool'),
            ('other', 'Other'),
        ],
        default='product'
    )
    
    # ØªÙˆØ¶ÛŒØ­Ø§Øª
    short_description = models.CharField(max_length=500, blank=True)
    description = models.TextField()
    long_description = models.TextField(blank=True)
    
    # â•â•â•â•â•â• Ù‚ÛŒÙ…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ â•â•â•â•â•â•
    price = models.DecimalField(max_digits=15, decimal_places=2, null=True, blank=True)
    original_price = models.DecimalField(max_digits=15, decimal_places=2, null=True, blank=True)
    discount_percentage = models.DecimalField(max_digits=5, decimal_places=2, null=True, blank=True)
    discount_amount = models.DecimalField(max_digits=15, decimal_places=2, null=True, blank=True)
    
    currency = models.CharField(
        max_length=10,
        choices=[
            ('USD', 'US Dollar'),
            ('EUR', 'Euro'),
            ('IRT', 'Iranian Toman'),
            ('IRR', 'Iranian Rial'),
        ],
        default='USD'
    )
    
    billing_period = models.CharField(
        max_length=20,
        choices=[
            ('one_time', 'One-time'),
            ('monthly', 'Monthly'),
            ('yearly', 'Yearly'),
        ],
        default='one_time'
    )
    
    # â•â•â•â•â•â• Ø¬Ø²Ø¦ÛŒØ§Øª â•â•â•â•â•â•
    brand = models.CharField(max_length=100, blank=True)
    category = models.CharField(max_length=100, blank=True)
    features = ArrayField(models.TextField(), default=list, blank=True)
    specifications = models.JSONField(default=dict, blank=True)
    
    # â•â•â•â•â•â• ØªØµØ§ÙˆÛŒØ± â•â•â•â•â•â•
    main_image = models.URLField(blank=True)
    images = ArrayField(models.URLField(), default=list, blank=True)
    
    # â•â•â•â•â•â• Ù…ÙˆØ¬ÙˆØ¯ÛŒ â•â•â•â•â•â•
    in_stock = models.BooleanField(default=True)
    stock_quantity = models.IntegerField(null=True, blank=True)
    
    # â•â•â•â•â•â• SEO â•â•â•â•â•â•
    meta_title = models.CharField(max_length=255, blank=True)
    meta_description = models.TextField(blank=True)
    keywords = ArrayField(models.CharField(max_length=50), default=list, blank=True)
    tags = ArrayField(models.CharField(max_length=50), default=list, blank=True)
    
    # â•â•â•â•â•â• Ù„ÛŒÙ†Ú© â•â•â•â•â•â•
    link = models.URLField(blank=True, help_text="Ù„ÛŒÙ†Ú© ØµÙØ­Ù‡ Ù…Ø­ØµÙˆÙ„")
    
    # â•â•â•â•â•â• Source Tracking (Ø¨Ø±Ø§ÛŒ AI extraction) â•â•â•â•â•â•
    source_website = models.ForeignKey(
        WebsiteSource,
        null=True,
        blank=True,
        on_delete=models.SET_NULL
    )
    source_page = models.ForeignKey(
        WebsitePage,
        null=True,
        blank=True,
        on_delete=models.SET_NULL
    )
    
    extraction_method = models.CharField(
        max_length=20,
        choices=[
            ('manual', 'Manual'),
            ('ai_auto', 'AI Auto'),
            ('ai_assisted', 'AI Assisted'),
        ],
        default='manual'
    )
    
    extraction_confidence = models.FloatField(
        default=1.0,
        help_text="Ø§Ø¹ØªÙ…Ø§Ø¯ AI Ø¨Ù‡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ (0-1)"
    )
    
    extraction_metadata = models.JSONField(
        default=dict,
        blank=True,
        help_text="Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÙ‡ (model, timestamp, etc.)"
    )
    
    # â•â•â•â•â•â• Status â•â•â•â•â•â•
    is_active = models.BooleanField(default=True)
    
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['user', 'is_active']),
            models.Index(fields=['extraction_method']),
            models.Index(fields=['source_website']),
        ]
    
    # â•â•â•â•â•â• Computed Properties â•â•â•â•â•â•
    
    @property
    def final_price(self):
        """Ù‚ÛŒÙ…Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø¹Ø¯ Ø§Ø² ØªØ®ÙÛŒÙ"""
        if not self.price:
            return None
        
        if self.discount_amount:
            return self.price - self.discount_amount
        elif self.discount_percentage:
            discount = (self.price * self.discount_percentage) / 100
            return self.price - discount
        
        return self.price
    
    @property
    def has_discount(self):
        """Ø¢ÛŒØ§ ØªØ®ÙÛŒÙ Ø¯Ø§Ø±Ù‡ØŸ"""
        return bool(self.discount_amount or self.discount_percentage)
    
    @property
    def discount_info(self):
        """Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØ®ÙÛŒÙ Ø¨Ù‡ ØµÙˆØ±Øª readable"""
        if self.discount_percentage:
            return f"{self.discount_percentage}% OFF"
        elif self.discount_amount:
            return f"-{self.discount_amount} {self.currency}"
        return None
    
    @property
    def is_auto_extracted(self):
        """Ø¢ÛŒØ§ Ø¨Ø§ AI Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ØŸ"""
        return self.extraction_method == 'ai_auto'
```

---

## 9. API Endpoints

### **ğŸ“ REST API**

**ÙØ§ÛŒÙ„:** `src/web_knowledge/urls.py`

```python
from django.urls import path, include
from rest_framework.routers import DefaultRouter
from web_knowledge import views

router = DefaultRouter()
router.register(r'websites', views.WebsiteSourceViewSet, basename='website')
router.register(r'pages', views.WebsitePageViewSet, basename='page')
router.register(r'products', views.ProductViewSet, basename='product')
router.register(r'qa-pairs', views.QAPairViewSet, basename='qa-pair')

urlpatterns = [
    path('', include(router.urls)),
]
```

### **ğŸ“ Key Endpoints**

#### **1. Ø´Ø±ÙˆØ¹ Ú©Ø±Ø§Ù„**

```http
POST /api/v1/web-knowledge/websites/create-and-crawl/

Request Body:
{
  "name": "WACACO Iran",
  "url": "https://www.wacaco.ir/",
  "max_pages": 50,
  "max_depth": 3,
  "auto_extract_products": true
}

Response:
{
  "website_id": "uuid",
  "crawl_job_id": "uuid",
  "status": "running",
  "message": "Crawl started"
}
```

#### **2. Ú†Ú© Ú©Ø±Ø¯Ù† ÙˆØ¶Ø¹ÛŒØª Ú©Ø±Ø§Ù„**

```http
GET /api/v1/web-knowledge/websites/{website_id}/crawl-status/

Response:
{
  "status": "completed",
  "pages_discovered": 30,
  "pages_crawled": 30,
  "pages_failed": 0,
  "qa_pairs_generated": 45,
  "products_extracted": 12,
  "progress_percentage": 100
}
```

#### **3. Ù„ÛŒØ³Øª Ù…Ø­ØµÙˆÙ„Ø§Øª**

```http
GET /api/v1/web-knowledge/products/

Query Parameters:
- website_id: ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§ÛŒØª
- has_discount: ÙÙ‚Ø· Ù…Ø­ØµÙˆÙ„Ø§Øª ØªØ®ÙÛŒÙâ€ŒØ¯Ø§Ø±
- min_price, max_price: Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù‚ÛŒÙ…Øª
- search: Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¹Ù†ÙˆØ§Ù† Ùˆ ØªÙˆØ¶ÛŒØ­Ø§Øª

Response:
{
  "count": 12,
  "results": [
    {
      "id": "uuid",
      "title": "Ù†Ø§Ù†ÙˆÙ¾Ø±Ø³Ùˆ Ù‚Ø±Ù…Ø² Ú¯Ø¯Ø§Ø²Ù‡",
      "price": "8249000",
      "original_price": "9799000",
      "currency": "IRT",
      "discount_percentage": "15.82",
      "has_discount": true,
      "brand": "WACACO",
      "features": ["Ù‚Ø§Ø¨Ù„ Ø­Ù…Ù„", "18 Ø¨Ø§Ø± ÙØ´Ø§Ø±", "..."],
      "in_stock": true,
      "link": "https://www.wacaco.ir/...",
      "extraction_method": "ai_auto",
      "is_auto_extracted": true
    },
    ...
  ]
}
```

#### **4. Ù¾Ø±Ø³ÛŒØ¯Ù† Ø³ÙˆØ§Ù„ Ø§Ø² AI**

```http
POST /api/v1/ai/ask-question/

Request Body:
{
  "question": "Ù‚ÛŒÙ…Øª Ù¾ÛŒÚ©ÙˆÙ¾Ø±Ø³Ùˆ Ú†Ù†Ø¯Ù‡ØŸ",
  "conversation_id": "uuid" // Ø§Ø®ØªÛŒØ§Ø±ÛŒ
}

Response:
{
  "success": true,
  "response": "Ù‚ÛŒÙ…Øª Ù¾ÛŒÚ©ÙˆÙ¾Ø±Ø³Ùˆ 13,989,000 ØªÙˆÙ…Ø§Ù† Ù‡Ø³Øª. Ø§ÛŒÙ† Ø¯Ø³ØªÚ¯Ø§Ù‡...",
  "response_time_ms": 2341,
  "metadata": {
    "intent": "pricing",
    "primary_source": "products",
    "avg_similarity": 0.89,
    "sources_used": [
      {
        "type": "product",
        "title": "Ù¾ÛŒÚ©ÙˆÙ¾Ø±Ø³Ùˆ",
        "score": 0.95
      }
    ]
  }
}
```

---

## 10. Database Schema

### **ğŸ“Š ER Diagram (Text)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WebsiteSource  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (UUID)       â”‚
â”‚ user_id (FK)    â”‚
â”‚ name            â”‚
â”‚ url             â”‚
â”‚ max_pages       â”‚
â”‚ auto_extract... â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 1:N
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WebsitePage    â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚    QAPair       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ 1:N   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (UUID)       â”‚       â”‚ id (UUID)       â”‚
â”‚ website_id (FK) â”‚       â”‚ user_id (FK)    â”‚
â”‚ url             â”‚       â”‚ page_id (FK)    â”‚
â”‚ title           â”‚       â”‚ question        â”‚
â”‚ cleaned_content â”‚       â”‚ answer          â”‚
â”‚ summary         â”‚       â”‚ source_type     â”‚
â”‚ processing_...  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 1:N
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Product      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (UUID)       â”‚
â”‚ user_id (FK)    â”‚
â”‚ source_page(FK) â”‚
â”‚ title           â”‚
â”‚ price           â”‚
â”‚ description     â”‚
â”‚ features        â”‚
â”‚ extraction_...  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Auto-sync via Signal
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TenantKnowledge       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (UUID)               â”‚
â”‚ user_id (FK)            â”‚
â”‚ chunk_type (enum)       â”‚â—€â”€â”€â”€â”€â”€â”
â”‚ source_id (UUID)        â”‚      â”‚
â”‚ full_text               â”‚      â”‚
â”‚ tldr                    â”‚      â”‚
â”‚ tldr_embedding (vector) â”‚â”€â”€â”€â”€â”€â”€â”¤
â”‚ full_embedding (vector) â”‚      â”‚ pgvector
â”‚ metadata (JSON)         â”‚      â”‚ for search
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”˜
```

---

## 11. Performance & Optimization

### **âš¡ Caching Strategy**

```python
# Redis cache Ø¨Ø±Ø§ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ
CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': 'redis://redis:6379/1',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
        }
    }
}

# Ù…Ø«Ø§Ù„: Cache Ú©Ø±Ø¯Ù† context retrieval
from django.core.cache import cache

def retrieve_with_cache(query, user):
    cache_key = f"context:{user.id}:{hash(query)}"
    
    cached = cache.get(cache_key)
    if cached:
        return cached
    
    result = ContextRetriever.retrieve_context(query, user, ...)
    
    # Cache Ø¨Ø±Ø§ÛŒ 1 Ø³Ø§Ø¹Øª
    cache.set(cache_key, result, 3600)
    
    return result
```

### **âš¡ Database Indexes**

```sql
-- pgvector index Ø¨Ø±Ø§ÛŒ fast similarity search
CREATE INDEX ON ai_model_tenantknowledge 
USING ivfflat (tldr_embedding vector_cosine_ops) 
WITH (lists = 100);

CREATE INDEX ON ai_model_tenantknowledge 
USING ivfflat (full_embedding vector_cosine_ops) 
WITH (lists = 100);

-- Regular indexes
CREATE INDEX idx_tenant_knowledge_user_type 
ON ai_model_tenantknowledge(user_id, chunk_type);

CREATE INDEX idx_product_user_active 
ON web_knowledge_product(user_id, is_active);
```

### **âš¡ Celery Optimization**

```python
# Task routing
CELERY_TASK_ROUTES = {
    'web_knowledge.tasks.crawl_website_task': {'queue': 'crawl'},
    'web_knowledge.tasks.process_page_content_task': {'queue': 'ai'},
}

# Worker pools
# Crawl: I/O bound â†’ eventlet
# AI: CPU bound â†’ prefork
celery -A core worker -Q crawl -P eventlet -c 10
celery -A core worker -Q ai -P prefork -c 4
```

---

## 12. Ø®Ù„Ø§ØµÙ‡ Ø¬Ø±ÛŒØ§Ù† Ú©Ø§Ù…Ù„

### **ğŸ”„ End-to-End Flow**

```
1. USER submits website URL
         â†“
2. WebsiteSource created in DB
         â†“
3. Celery task: crawl_website_task
   - Fetch HTML
   - Parse with BeautifulSoup
   - Clean with Readability
   - Save as WebsitePage (status: pending)
         â†“
4. For each WebsitePage â†’ Celery task: process_page_content_task
         â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  4a. Q&A Generation            â”‚
   â”‚  - Prompt to Gemini 2.5 Flash  â”‚
   â”‚  - Parse JSON response         â”‚
   â”‚  - Save as QAPair              â”‚
   â”‚  - Ingest to TenantKnowledge   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  4b. Product Extraction        â”‚
   â”‚  - Pre-filter (rule-based)     â”‚
   â”‚  - AI extraction (Gemini 2.5 Pro)â”‚
   â”‚  - Save as Product             â”‚
   â”‚  - Signal â†’ TenantKnowledge    â”‚
   â”‚    (with OpenAI embedding)     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
5. TenantKnowledge chunks ready for search
         â†“
6. USER asks question in chatbot
         â†“
7. Query â†’ OpenAI embedding
         â†“
8. pgvector search â†’ Find top matches
         â†“
9. Matched chunks â†’ Gemini prompt
         â†“
10. Gemini generates response with context
         â†“
11. Response returned to USER
```

---

## ğŸ“š Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø±ØªØ¨Ø·

- **[DEPLOYMENT_INSTRUCTIONS.md](./DEPLOYMENT_INSTRUCTIONS.md)** - Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ deploy
- **[AI_RESPONSE_ALGORITHM_ARCHITECTURE.md](./AI_RESPONSE_ALGORITHM_ARCHITECTURE.md)** - Ù…Ø¹Ù…Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ AI chatbot
- **[PERSONA_TONE_IMPLEMENTATION_GUIDE.md](./PERSONA_TONE_IMPLEMENTATION_GUIDE.md)** - Ø´Ø®ØµÛŒØªâ€ŒØ³Ø§Ø²ÛŒ AI

---

## ğŸ¯ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ

Ø§ÛŒÙ† Ø³ÛŒØ³ØªÙ… ÛŒÚ© **Ù¾Ø§ÛŒÙ¾Ù„Ø§ÛŒÙ† Ú©Ø§Ù…Ù„ Ø§Ø² web crawling ØªØ§ AI serving** Ø±Ùˆ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù‡:

âœ… **Automated**: Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø®ÙˆØ¯Ú©Ø§Ø± (crawl â†’ extract â†’ index â†’ serve)  
âœ… **Scalable**: Ø¨Ø§ Celery + Redis + PostgreSQL  
âœ… **Accurate**: Gemini 2.5 Pro Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ØŒ OpenAI Ø¨Ø±Ø§ÛŒ embedding  
âœ… **Fast**: pgvector Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…ÛŒÙ„ÛŒÙˆÙ†â€ŒÙ‡Ø§ vector Ø¯Ø± milliseconds  
âœ… **Production-Ready**: Ø¨Ø§ error handlingØŒ retry logicØŒ monitoring

**Ù†ØªÛŒØ¬Ù‡:** ÛŒÚ© chatbot Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆÙ†Ù‡ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø§Ø² Ø±ÙˆÛŒ Ù…Ø­ØªÙˆØ§ÛŒ Ø³Ø§ÛŒØª Ø´Ù…Ø§ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡ØŒ Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ manual data entry! ğŸš€

