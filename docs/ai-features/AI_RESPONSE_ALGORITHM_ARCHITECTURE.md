# ğŸ¤– Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ - Ù…Ø¹Ù…Ø§Ø±ÛŒ ÙÙ†ÛŒ Ú©Ø§Ù…Ù„

> **Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙÙ†ÛŒ Ø¬Ø§Ù…Ø¹ Ø³ÛŒØ³ØªÙ… Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ AI Ø¨Ø§ Ù…Ø¹Ù…Ø§Ø±ÛŒ Lean RAG v2.1**
> 
> Ø§ÛŒÙ† Ø³Ù†Ø¯ Ø´Ø§Ù…Ù„ ØªÙ…Ø§Ù…ÛŒ Ø¬Ø²Ø¦ÛŒØ§Øª ÙÙ†ÛŒØŒ Ù…Ø¹Ù…Ø§Ø±ÛŒØŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ØŒ Ùˆ Ù†Ø­ÙˆÙ‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ… Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ FIKO Ø§Ø³Øª.

**Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡:** FIKO AI Team  
**Ø¢Ø®Ø±ÛŒÙ† Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ:** Ø§Ú©ØªØ¨Ø± 2025  
**Ù†Ø³Ø®Ù‡:** 2.1

---

## ğŸ“‹ ÙÙ‡Ø±Ø³Øª Ù…Ø·Ø§Ù„Ø¨

1. [Ù†Ù…Ø§ÛŒ Ú©Ù„ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ](#-Ù†Ù…Ø§ÛŒ-Ú©Ù„ÛŒ-Ù…Ø¹Ù…Ø§Ø±ÛŒ)
2. [Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ (Lean RAG)](#-Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…-Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ-lean-rag)
3. [Ø³ÛŒØ³ØªÙ… Chunking Ùˆ Embedding](#-Ø³ÛŒØ³ØªÙ…-chunking-Ùˆ-embedding)
4. [Knowledge Sources Ùˆ Crawling](#-knowledge-sources-Ùˆ-crawling)
5. [Query Router (Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ÛŒ Ø³ÙˆØ§Ù„)](#-query-router-Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ÛŒ-Ø³ÙˆØ§Ù„)
6. [Context Retriever (RAG Ø¨Ø§ pgvector)](#-context-retriever-rag-Ø¨Ø§-pgvector)
7. [Token Management](#-token-management)
8. [Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ](#-Ù…Ø«Ø§Ù„Ù‡Ø§ÛŒ-Ø¹Ù…Ù„ÛŒ)
9. [Monitoring Ùˆ Performance](#-monitoring-Ùˆ-performance)

---

## ğŸ—ï¸ Ù†Ù…Ø§ÛŒ Ú©Ù„ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ

### Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø³ÛŒØ³ØªÙ… (System Architecture)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           FIKO AI Response System                        â”‚
â”‚                         Lean RAG Architecture v2.1                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  User Query   â”‚ "Ù‚ÛŒÙ…Øª Ù¾Ù„Ù† Ø´Ù…Ø§ Ú†Ù†Ø¯Ù‡ØŸ"
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  1. Query Router                â”‚  â† Rule-based + Keyword matching
    â”‚  (Intent Classification)        â”‚
    â”‚  - Multilingual (FA/EN/AR/TR)   â”‚
    â”‚  - Output: intent, confidence   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ intent="pricing", conf=0.85
                 â”‚ primary_source="faq"
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  2. Embedding Service            â”‚  â† OpenAI text-embedding-3-small
    â”‚  (Semantic Vector Generation)    â”‚    (1536 dimensions)
    â”‚  - Primary: OpenAI              â”‚
    â”‚  - Fallback: Gemini             â”‚
    â”‚  - Cache: Redis (30 days)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ query_embedding=[0.023, -0.145, ...]
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  3. Context Retriever            â”‚  â† pgvector Cosine Similarity
    â”‚  (RAG - Semantic Search)         â”‚
    â”‚  - TenantKnowledge DB            â”‚
    â”‚  - Top-K chunks (K=5)            â”‚
    â”‚  - Min similarity: 0.1           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ primary_chunks=[FAQ1, FAQ2, FAQ3]
                 â”‚ secondary_chunks=[Product1]
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  4. Session Memory Manager       â”‚  â† Conversation Context
    â”‚  (Conversation Intelligence)     â”‚
    â”‚  - Rolling summary (10+ msgs)    â”‚
    â”‚  - Recent messages (last 3)      â”‚
    â”‚  - Cached summaries              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ conversation_context="..."
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  5. Token Budget Controller      â”‚  â† Strict 1500 Token Limit
    â”‚  (Prompt Trimming)               â”‚
    â”‚  - System: 200 tokens            â”‚
    â”‚  - Memory: 150 tokens            â”‚
    â”‚  - Context: 800 tokens           â”‚
    â”‚  - Query: 350 tokens             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ trimmed_prompt (â‰¤1500 tokens)
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  6. Gemini Chat Service          â”‚  â† Gemini 2.5 Flash API
    â”‚  (AI Response Generation)        â”‚
    â”‚  - Model: gemini-2.5-flash       â”‚
    â”‚  - Temperature: 0.7              â”‚
    â”‚  - Max tokens: 3000              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ ai_response="Ù¾Ù„Ù† Ù…Ø§ 3 Ù†ÙˆØ¹ Ø¯Ø§Ø±Ù‡..."
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  7. Response Handler             â”‚  â† Format & Send
    â”‚  - Token billing                 â”‚
    â”‚  - Usage tracking                â”‚
    â”‚  - Platform routing              â”‚
    â”‚    (Telegram/Instagram/Web)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  User Reply   â”‚ Ù¾Ù„Ù† Pro Ø¨Ù‡ Ù‚ÛŒÙ…Øª $29
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ (Lean RAG)

### Ù…Ø¹Ù…Ø§Ø±ÛŒ Lean RAG v2.1

**Ù‡Ø¯Ù:** Ú©Ø§Ù‡Ø´ Ù‡Ø²ÛŒÙ†Ù‡ ØªÙˆÚ©Ù† Ø¨Ù‡ **â‰¤1500 ØªÙˆÚ©Ù† ÙˆØ±ÙˆØ¯ÛŒ** Ø¨Ø§ Ø­ÙØ¸ Ø¯Ù‚Øª Ù¾Ø§Ø³Ø® **â‰¥90%**

### Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…:

#### Ù…Ø±Ø­Ù„Ù‡ 1: Query Routing (Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ÛŒ Ø³ÙˆØ§Ù„)

**Ú©Ø¯:** `src/AI_model/services/query_router.py`

```python
routing = QueryRouter.route_query(customer_message, user=self.user)

# Output:
{
    'intent': 'pricing',               # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†ÛŒØª Ú©Ø§Ø±Ø¨Ø±
    'confidence': 0.85,                # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† (0-1)
    'primary_source': 'faq',           # Ù…Ù†Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¬Ø³ØªØ¬Ùˆ
    'secondary_sources': ['products'], # Ù…Ù†Ø§Ø¨Ø¹ Ø¬Ø§Ù†Ø¨ÛŒ
    'token_budgets': {
        'primary': 800,                # Ø¨ÙˆØ¯Ø¬Ù‡ ØªÙˆÚ©Ù† Ø¨Ø±Ø§ÛŒ Ù…Ù†Ø¨Ø¹ Ø§ØµÙ„ÛŒ
        'secondary': 300               # Ø¨ÙˆØ¯Ø¬Ù‡ ØªÙˆÚ©Ù† Ø¨Ø±Ø§ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ø¬Ø§Ù†Ø¨ÛŒ
    },
    'keywords_matched': ['Ù‚ÛŒÙ…Øª', 'Ù¾Ù„Ù†'],
    'method': 'keyword_based'
}
```

**Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ÛŒ:**

1. **ØªØ´Ø®ÛŒØµ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ (Multilingual):**
   - FA: Ù‚ÛŒÙ…ØªØŒ Ù‡Ø²ÛŒÙ†Ù‡ØŒ Ù¾Ù„Ù†ØŒ Ø§Ø´ØªØ±Ø§Ú©ØŒ Ø®Ø±ÛŒØ¯
   - EN: price, cost, plan, subscription, buy
   - AR: Ø³Ø¹Ø±ØŒ ØªÙƒÙ„ÙØ©ØŒ Ø®Ø·Ø©ØŒ Ø§Ø´ØªØ±Ø§Ùƒ
   - TR: fiyat, maliyet, plan, abonelik

2. **Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Intent:**
   ```python
   for intent in ['pricing', 'product', 'howto', 'contact']:
       score = sum(keyword_weight for keyword in matched_keywords)
       intent_scores[intent] = score
   
   best_intent = max(intent_scores, key=intent_scores.get)
   confidence = max_score / total_score
   ```

3. **Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ù†Ø§Ø¨Ø¹ Ø¨Ø± Ø§Ø³Ø§Ø³ Intent:**
   - `pricing` â†’ Primary: FAQ, Secondary: Products + Manual
   - `product` â†’ Primary: Products, Secondary: FAQ + Website
   - `howto` â†’ Primary: Manual, Secondary: FAQ + Website
   - `contact` â†’ Primary: Manual, Secondary: Website
   - `general` â†’ Primary: FAQ, Secondary: Manual

**Ù…Ø²Ø§ÛŒØ§:**
- âœ… Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ AI (Ø³Ø±ÛŒØ¹ØŒ Ø±Ø§ÛŒÚ¯Ø§Ù†)
- âœ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² 4 Ø²Ø¨Ø§Ù† (FA/EN/AR/TR)
- âœ… Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ… Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ (`IntentKeyword`, `IntentRouting`)
- âœ… Cache Ø´Ø¯Ù‡ (1 Ø³Ø§Ø¹Øª)

---

#### Ù…Ø±Ø­Ù„Ù‡ 2: Embedding Generation (ØªÙˆÙ„ÛŒØ¯ Ø¨Ø±Ø¯Ø§Ø± Ù…Ø¹Ù†Ø§ÛŒÛŒ)

**Ú©Ø¯:** `src/AI_model/services/embedding_service.py`

```python
embedding_service = EmbeddingService()
query_embedding = embedding_service.get_embedding(
    text="Ù‚ÛŒÙ…Øª Ù¾Ù„Ù† Ø´Ù…Ø§ Ú†Ù†Ø¯Ù‡ØŸ",
    task_type="retrieval_query"
)
# Output: [0.0234, -0.1456, 0.0892, ..., 0.0234]  # 1536 dimensions
```

**Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Embedding:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Embedding Service Strategy                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Check Redis Cache (30 days TTL)
   â”œâ”€ Hit? â†’ Return cached embedding âœ…
   â””â”€ Miss? â†’ Continue â¬‡ï¸

2. Try OpenAI API (Primary)
   Model: text-embedding-3-small
   Dimensions: 1536
   Languages: 100+
   Cost: $0.02 / 1M tokens
   â”œâ”€ Success? â†’ Cache & Return âœ…
   â””â”€ Fail? â†’ Continue â¬‡ï¸

3. Try Gemini API (Fallback)
   Model: text-embedding-004
   Dimensions: 768
   Languages: 100+
   Cost: Free (1500 req/day)
   â”œâ”€ Success? â†’ Cache & Return âœ…
   â””â”€ Fail? â†’ Continue â¬‡ï¸

4. Return None â†’ Caller uses BM25 âš ï¸
```

**Ú†Ø±Ø§ text-embedding-3-smallØŸ**
- PostgreSQL 15 ivfflat index Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø¯Ø§Ø±Ø¯: max 2000 dimensions
- text-embedding-3-large = 3072 dims (Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ âŒ)
- text-embedding-3-small = 1536 dims (Ù…Ù†Ø§Ø³Ø¨ âœ…)
- Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§ØªØ±ØŒ Ù‡Ø²ÛŒÙ†Ù‡ Ú©Ù…ØªØ±ØŒ Ø¯Ù‚Øª Ø¹Ø§Ù„ÛŒ

**Ù…Ø«Ø§Ù„ Cache Key:**
```python
cache_key = f"emb:v2:{md5(task_type + text)[:20]}"
# Example: "emb:v2:a3f5d8c2b1e4f6a7b8c9"
cache.set(cache_key, embedding, timeout=30*24*60*60)  # 30 days
```

---

#### Ù…Ø±Ø­Ù„Ù‡ 3: Context Retrieval (Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨Ø§ RAG)

**Ú©Ø¯:** `src/AI_model/services/context_retriever.py`

**Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…:**

```python
retrieval_result = ContextRetriever.retrieve_context(
    query=customer_message,
    user=self.user,
    primary_source='faq',           # Ø§Ø² Query Router
    secondary_sources=['products'], # Ø§Ø² Query Router
    primary_budget=800,             # Ø¨ÙˆØ¯Ø¬Ù‡ ØªÙˆÚ©Ù†
    secondary_budget=300,
    routing_info=routing
)

# Output:
{
    'primary_context': [
        {
            'title': 'Ù‚ÛŒÙ…Øª Ù¾Ù„Ù†â€ŒÙ‡Ø§ÛŒ Ø§Ø´ØªØ±Ø§Ú©ÛŒ',
            'content': 'Ù…Ø§ 3 Ù¾Ù„Ù† Ø¯Ø§Ø±ÛŒÙ…: Starter ($14)ØŒ Pro ($29)ØŒ Enterprise (Ø³ÙØ§Ø±Ø´ÛŒ)',
            'type': 'faq',
            'score': 0.892,  # Cosine similarity
            'source_id': UUID('...')
        },
        ...  # Top 5 chunks
    ],
    'secondary_context': [
        {
            'title': 'Ù¾Ù„Ù† Professional',
            'content': 'Ù¾Ù„Ù† Pro Ø¨Ø§ Ù‚ÛŒÙ…Øª $29/Ù…Ø§Ù‡ Ø´Ø§Ù…Ù„ 5000 ØªÙˆÚ©Ù†ØŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ...',
            'type': 'product',
            'score': 0.765
        },
        ...  # Top 3 chunks
    ],
    'sources_used': ['faq', 'products'],
    'total_chunks': 8,
    'retrieval_method': 'semantic_search'
}
```

**Ù†Ø­ÙˆÙ‡ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± PostgreSQL Ø¨Ø§ pgvector:**

```sql
-- Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨Ø§ Cosine Similarity
SELECT 
    id,
    section_title,
    full_text,
    chunk_type,
    (1 - (tldr_embedding <=> %s::vector)) AS similarity  -- Cosine similarity
FROM 
    ai_model_tenantknowledge
WHERE 
    user_id = %s
    AND chunk_type = 'faq'
    AND tldr_embedding IS NOT NULL
ORDER BY 
    tldr_embedding <=> %s::vector  -- Cosine distance (lower is better)
LIMIT 10;
```

**Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Two-Stage Retrieval:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Two-Stage Retrieval (TL;DR â†’ Full Text)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Stage 1: Search by TL;DR Embeddings (Efficient)
â”œâ”€ Query: tldr_embedding <=> query_embedding
â”œâ”€ Get Top 10 chunks
â””â”€ Filter: similarity >= 0.1

Stage 2: Use Full Text for Context
â”œâ”€ Return full_text (not TL;DR)
â”œâ”€ Apply token budget trimming
â””â”€ Max 5 chunks for primary source
```

**Ú†Ø±Ø§ TL;DR â†’ Full TextØŸ**
- TL;DR: Ú©ÙˆØªØ§Ù‡ (80-120 Ú©Ù„Ù…Ù‡) â†’ Ø¬Ø³ØªØ¬Ùˆ Ø³Ø±ÛŒØ¹â€ŒØªØ±
- Full Text: Ú©Ø§Ù…Ù„ (300-500 Ú©Ù„Ù…Ù‡) â†’ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±
- ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ 40% Ø²Ù…Ø§Ù† Ø¬Ø³ØªØ¬Ùˆ

---

#### Ù…Ø±Ø­Ù„Ù‡ 4: Conversation Memory (Ø­Ø§ÙØ¸Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡)

**Ú©Ø¯:** `src/AI_model/services/session_memory_manager.py`

**Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Rolling Summary:**

```python
if message_count > 10:
    # Summarize old messages (exclude last 5)
    summary = gemini.generate_content(f"""
    Summarize this conversation in 2-3 sentences:
    {conversation_history}
    """, max_output_tokens=150)
    
    # Cache summary (1 hour)
    cache.set(f"conv_summary:{conversation_id}", summary, 3600)

# Build context
conversation_context = f"""
Summary: {summary}  # 100-150 words

Recent Messages:
- User: {recent_msg_1}
- AI: {recent_msg_2}
- User: {recent_msg_3}
"""
```

**Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡:**
- **10 Ù¾ÛŒØ§Ù… Ø§ÙˆÙ„:** ØªÙ…Ø§Ù… ØªØ§Ø±ÛŒØ®Ú†Ù‡ (Ø¨Ø¯ÙˆÙ† Ø®Ù„Ø§ØµÙ‡)
- **10+ Ù¾ÛŒØ§Ù…:** Ø®Ù„Ø§ØµÙ‡ (msgs 1-N-5) + Ú©Ø§Ù…Ù„ (msgs N-4 ØªØ§ N)
- **Cache:** 1 Ø³Ø§Ø¹Øª (ØªØ§ Ø²Ù…Ø§Ù† ØªØºÛŒÛŒØ± Ù…Ú©Ø§Ù„Ù…Ù‡)
- **Token Budget:** 150 ØªÙˆÚ©Ù†

---

#### Ù…Ø±Ø­Ù„Ù‡ 5: Token Budget Control (Ù…Ø¯ÛŒØ±ÛŒØª Ø¨ÙˆØ¯Ø¬Ù‡ ØªÙˆÚ©Ù†)

**Ú©Ø¯:** `src/AI_model/services/token_budget_controller.py`

**Ù‡Ø¯Ù:** Ø­Ø¯Ø§Ú©Ø«Ø± 1500 ØªÙˆÚ©Ù† ÙˆØ±ÙˆØ¯ÛŒ

**ØªØ®ØµÛŒØµ Ø¨ÙˆØ¯Ø¬Ù‡:**

```python
TOKEN_BUDGET = 1500  # Total input tokens

BUDGET_ALLOCATION = {
    'system_prompt': 200,    # 13.3%  - System instructions
    'customer_info': 50,     # 3.3%   - Name, phone, source
    'conversation': 150,     # 10.0%  - Memory + recent messages
    'primary_context': 800,  # 53.3%  - Main knowledge chunks
    'secondary_context': 300 # 20.0%  - Supplementary chunks
}

# Query is variable (usually 50-200 tokens)
```

**Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Trimming:**

```python
def trim_to_budget(components):
    total_tokens = 0
    
    # 1. System Prompt (Ø¶Ø±ÙˆØ±ÛŒ - Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ú©ÙˆØªØ§Ù‡ Ø´ÙˆØ¯)
    system_tokens = count_tokens(components['system_prompt'])
    total_tokens += min(system_tokens, 200)
    
    # 2. Customer Info (Ø¶Ø±ÙˆØ±ÛŒ)
    customer_tokens = count_tokens(components['customer_info'])
    total_tokens += min(customer_tokens, 50)
    
    # 3. Conversation (Ù‚Ø§Ø¨Ù„ Ú©Ø§Ù‡Ø´)
    conv_tokens = count_tokens(components['conversation'])
    if total_tokens + conv_tokens > BUDGET:
        conv_tokens = BUDGET - total_tokens
        # Trim oldest messages first
        components['conversation'] = trim_text(
            components['conversation'], 
            max_tokens=conv_tokens
        )
    total_tokens += conv_tokens
    
    # 4. Primary Context (Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ø¨Ø®Ø´)
    for chunk in components['primary_context']:
        chunk_tokens = count_tokens(chunk['content'])
        if total_tokens + chunk_tokens > BUDGET:
            # Trim this chunk or skip
            remaining = BUDGET - total_tokens
            if remaining > 100:
                chunk['content'] = trim_text(chunk['content'], remaining)
                total_tokens += remaining
            break
        total_tokens += chunk_tokens
    
    # 5. Secondary Context (Ø§Ú¯Ø± ÙØ¶Ø§ Ù…Ø§Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ø¯)
    for chunk in components['secondary_context']:
        chunk_tokens = count_tokens(chunk['content'])
        if total_tokens + chunk_tokens > BUDGET:
            break
        total_tokens += chunk_tokens
    
    # 6. User Query (Ù‡Ù…ÛŒØ´Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
    query_tokens = count_tokens(components['user_query'])
    total_tokens += query_tokens
    
    return trimmed_components, total_tokens
```

---

#### Ù…Ø±Ø­Ù„Ù‡ 6: Prompt Building (Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª Ù†Ù‡Ø§ÛŒÛŒ)

**Ú©Ø¯:** `src/AI_model/services/gemini_service.py` â†’ `_build_prompt()`

**Ø³Ø§Ø®ØªØ§Ø± Prompt:**

```
SYSTEM: {combined_prompt}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{combined_prompt} = 
    - Mother Prompt (auto_prompt from GeneralSettings)
    - Manual Prompt (user's manual_prompt)
    - Business Prompt (industry-specific guidelines)
    - Greeting Rule (smart greeting logic)

Customer: Name: Ø§Ø­Ù…Ø¯ Ù…Ø­Ù…Ø¯ÛŒ, Phone: 09123456789, Source: telegram

CONVERSATION HISTORY:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Summary: Ù…Ø´ØªØ±ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ Ø³ÙˆØ§Ù„ Ú©Ø±Ø¯Ù‡ Ùˆ Ù…Ø§ Ù¾Ù„Ù†â€ŒÙ‡Ø§ Ø±Ùˆ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯ÛŒÙ….

Recent Messages:
- User: Ù¾Ù„Ù† Pro Ú†Ù‡ Ø§Ù…Ú©Ø§Ù†Ø§ØªÛŒ Ø¯Ø§Ø±Ù‡ØŸ
- AI: Ù¾Ù„Ù† Pro Ø´Ø§Ù…Ù„ 5000 ØªÙˆÚ©Ù†ØŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ 24/7ØŒ Ùˆ...
- User: Ù‚ÛŒÙ…Øª Ù¾Ù„Ù† Ø´Ù…Ø§ Ú†Ù†Ø¯Ù‡ØŸ

KNOWLEDGE BASE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
**Ù‚ÛŒÙ…Øª Ù¾Ù„Ù†â€ŒÙ‡Ø§ÛŒ Ø§Ø´ØªØ±Ø§Ú©ÛŒ**
Ù…Ø§ 3 Ù¾Ù„Ù† Ø§Ø´ØªØ±Ø§Ú©ÛŒ Ø¯Ø§Ø±ÛŒÙ…:
1. Starter: $14/Ù…Ø§Ù‡ - Ø´Ø§Ù…Ù„ 1000 ØªÙˆÚ©Ù†
2. Pro: $29/Ù…Ø§Ù‡ - Ø´Ø§Ù…Ù„ 5000 ØªÙˆÚ©Ù†
3. Enterprise: Ù‚ÛŒÙ…Øª Ø³ÙØ§Ø±Ø´ÛŒ - ØªÙˆÚ©Ù† Ù†Ø§Ù…Ø­Ø¯ÙˆØ¯

**Ø§Ù…Ú©Ø§Ù†Ø§Øª Ù¾Ù„Ù† Professional**
Ù¾Ù„Ù† Pro Ø´Ø§Ù…Ù„:
- 5000 ØªÙˆÚ©Ù† AI Ù…Ø§Ù‡Ø§Ù†Ù‡
- Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¯Ø§Ø± 24/7
- API access
- Advanced analytics
- Ù†ØµØ¨ Ø±ÙˆÛŒ Ø¯Ø§Ù…Ù†Ù‡ Ø´Ø®ØµÛŒ

ADDITIONAL INFO:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
- Ù…Ø­ØµÙˆÙ„: Ù¾Ù„Ù† Professional - Ù‚ÛŒÙ…Øª $29ØŒ Ù„ÛŒÙ†Ú©: https://fiko.net/pricing

CUSTOMER QUESTION:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Ù‚ÛŒÙ…Øª Ù¾Ù„Ù† Ø´Ù…Ø§ Ú†Ù†Ø¯Ù‡ØŸ

INSTRUCTION:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Answer using the knowledge base above. Be accurate, helpful, and natural.
```

**Token Count Example:**
```
System: 180 tokens
Customer: 25 tokens
Conversation: 120 tokens
Knowledge Base: 650 tokens
Additional: 180 tokens
Query: 12 tokens
Instruction: 35 tokens
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 1202 tokens âœ… (under 1500)
```

---

#### Ù…Ø±Ø­Ù„Ù‡ 7: AI Response Generation (ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®)

**Ú©Ø¯:** `src/AI_model/services/gemini_service.py` â†’ `generate_response()`

**ØªÙ†Ø¸ÛŒÙ…Ø§Øª Gemini API:**

```python
model = genai.GenerativeModel(
    model_name='gemini-2.5-flash',  # ÛŒØ§ gemini-1.5-flash
    generation_config={
        'temperature': 0.7,           # Ø®Ù„Ø§Ù‚ÛŒØª Ù…ØªÙˆØ³Ø·
        'max_output_tokens': 3000,    # Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ù¾Ø§Ø³Ø®
        'top_p': 0.8,
        'top_k': 40
    }
)

# Safety Settings (Ø¨Ø±Ø§ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§Ø±Ø³ÛŒ/Ø¹Ø±Ø¨ÛŒ)
safety_settings = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

response = model.generate_content(
    prompt,
    safety_settings=safety_settings
)

# Extract token usage
prompt_tokens = response.usage_metadata.prompt_token_count
completion_tokens = response.usage_metadata.candidates_token_count
total_tokens = prompt_tokens + completion_tokens
```

**Ù¾Ø§Ø³Ø® Ø®Ø±ÙˆØ¬ÛŒ:**

```python
{
    'success': True,
    'response': 'Ø³Ù„Ø§Ù… Ø§Ø­Ù…Ø¯! ğŸ‘‹\n\nÙ…Ø§ 3 Ù¾Ù„Ù† Ø§Ø´ØªØ±Ø§Ú©ÛŒ Ø¯Ø§Ø±ÛŒÙ…:\n\n1. **Starter** - $14/Ù…Ø§Ù‡\n- Ø´Ø§Ù…Ù„ 1000 ØªÙˆÚ©Ù† AI\n\n2. **Pro** - $29/Ù…Ø§Ù‡\n- Ø´Ø§Ù…Ù„ 5000 ØªÙˆÚ©Ù† AI\n- Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ 24/7\n- API access\n\n3. **Enterprise** - Ù‚ÛŒÙ…Øª Ø³ÙØ§Ø±Ø´ÛŒ\n- ØªÙˆÚ©Ù† Ù†Ø§Ù…Ø­Ø¯ÙˆØ¯\n\nÙ¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…Ù† Ø¨Ø±Ø§ÛŒ Ø´Ù…Ø§ Ù¾Ù„Ù† Pro Ù‡Ø³Øª Ú©Ù‡ Ø§Ù…Ú©Ø§Ù†Ø§Øª Ú©Ø§Ù…Ù„ÛŒ Ø¯Ø§Ø±Ù‡! ğŸ˜Š',
    'response_time_ms': 2340,
    'metadata': {
        'model_used': 'gemini-2.5-flash',
        'prompt_tokens': 1202,
        'completion_tokens': 187,
        'total_tokens': 1389,
        'timestamp': '2025-10-10T15:30:45Z'
    }
}
```

---

## ğŸ§© Ø³ÛŒØ³ØªÙ… Chunking Ùˆ Embedding

### Ù…Ø¹Ù…Ø§Ø±ÛŒ Hybrid Auto-Chunking

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Hybrid Chunking: Real-Time + Batch Reconciliation    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Real-Time Chunking (Django Signals)          Nightly Reconciliation
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”          â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                                            
QAPair.save()                                Every 24h @ 3 AM UTC
   â”‚                                              â”‚
   â”œâ”€â†’ post_save signal                          â”œâ”€â†’ Celery Beat
   â”‚                                              â”‚
   â”œâ”€â†’ Debounce 5s                               â”œâ”€â†’ Scan all sources
   â”‚                                              â”‚
   â””â”€â†’ chunk_qapair_async.delay()                â”œâ”€â†’ Delete orphaned chunks
        â”‚                                         â”‚
        â”œâ”€â†’ IncrementalChunker.chunk_qapair()    â”œâ”€â†’ Create missing chunks
        â”‚   â”‚                                     â”‚
        â”‚   â”œâ”€â†’ Delete old chunk (idempotent)    â””â”€â†’ Fix missing embeddings
        â”‚   â”œâ”€â†’ Generate TL;DR
        â”‚   â”œâ”€â†’ Generate embeddings
        â”‚   â”‚   â”œâ”€â†’ OpenAI API
        â”‚   â”‚   â””â”€â†’ Gemini fallback
        â”‚   â””â”€â†’ Create TenantKnowledge chunk
        â”‚
        â””â”€â†’ Cache invalidation
```

### TenantKnowledge Model (Vector Store)

**Ú©Ø¯:** `src/AI_model/models.py`

```python
class TenantKnowledge(models.Model):
    """
    Vector store Ø¨Ø±Ø§ÛŒ RAG
    Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² pgvector Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ
    """
    
    # Primary key
    id = models.UUIDField(primary_key=True, default=uuid.uuid4)
    
    # Owner (Multi-tenancy)
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    
    # Source reference
    chunk_type = models.CharField(
        max_length=20,
        choices=[
            ('faq', 'FAQ'),
            ('manual', 'Manual Prompt'),
            ('product', 'Product'),
            ('website', 'Website Page'),
        ]
    )
    source_id = models.UUIDField(
        help_text="Reference to original FAQ/Product/WebsitePage ID"
    )
    
    # Hierarchical structure (Ø¨Ø±Ø§ÛŒ Manual Prompt Ø¨Ø²Ø±Ú¯)
    document_id = models.UUIDField(
        help_text="Group chunks from same document"
    )
    section_title = models.TextField()
    
    # Content
    full_text = models.TextField()  # 300-500 Ú©Ù„Ù…Ù‡
    tldr = models.TextField()       # 80-120 Ú©Ù„Ù…Ù‡ (Ø®Ù„Ø§ØµÙ‡)
    
    # Embeddings (pgvector)
    tldr_embedding = VectorField(
        dimensions=1536,  # OpenAI text-embedding-3-small
        help_text="Fast retrieval (TL;DR)"
    )
    full_embedding = VectorField(
        dimensions=1536,
        help_text="Full content (if needed)"
    )
    
    # Metadata
    word_count = models.IntegerField()
    language = models.CharField(max_length=10)  # fa, en, ar, tr
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        indexes = [
            models.Index(fields=['user', 'chunk_type']),
            models.Index(fields=['user', 'source_id']),
            # pgvector index (created via SQL migration)
            # CREATE INDEX ON ai_model_tenantknowledge 
            # USING ivfflat (tldr_embedding vector_cosine_ops)
            # WITH (lists = 100);
        ]
```

### Incremental Chunker (Real-Time)

**Ú©Ø¯:** `src/AI_model/services/incremental_chunker.py`

**Ù…Ø«Ø§Ù„: Chunk Ú©Ø±Ø¯Ù† QAPair**

```python
chunker = IncrementalChunker(user=user)

def chunk_qapair(qa):
    # 1. Delete old chunk (idempotent)
    TenantKnowledge.objects.filter(
        user=user,
        source_id=qa.id,
        chunk_type='faq'
    ).delete()
    
    # 2. Build full text
    full_text = f"Q: {qa.question}\n\nA: {qa.answer}"
    
    # 3. Generate TL;DR (extractive)
    tldr = _extract_tldr(full_text, max_words=100)
    # Example TL;DR: "Q: Ù‚ÛŒÙ…Øª Ù¾Ù„Ù† Ú†Ù†Ø¯Ù‡ØŸ\n\nA: Ù…Ø§ 3 Ù¾Ù„Ù† Ø¯Ø§Ø±ÛŒÙ…: Starter ($14), Pro ($29), Enterprise..."
    
    # 4. Generate embeddings
    embedding_service = EmbeddingService()
    tldr_embedding = embedding_service.get_embedding(tldr)          # 1536 dims
    full_embedding = embedding_service.get_embedding(full_text)     # 1536 dims
    
    # 5. Create chunk
    TenantKnowledge.objects.create(
        user=user,
        chunk_type='faq',
        source_id=qa.id,
        section_title=qa.question[:200],
        full_text=full_text,
        tldr=tldr,
        tldr_embedding=tldr_embedding,
        full_embedding=full_embedding,
        word_count=len(full_text.split()),
        language=detect_language(full_text)  # 'fa', 'en', etc.
    )
```

**Ù…Ø«Ø§Ù„: Chunk Ú©Ø±Ø¯Ù† Manual Prompt (Ø¨Ø²Ø±Ú¯)**

```python
def chunk_manual_prompt():
    ai_prompts = AIPrompts.objects.get(user=user)
    manual_text = ai_prompts.manual_prompt  # Ù…Ù…Ú©Ù† Ø§Ø³Øª 10,000+ Ú©Ù„Ù…Ù‡ Ø¨Ø§Ø´Ø¯
    
    # Split into chunks (500 words each)
    chunks = _chunk_text(manual_text, max_words=500)
    # Example: 20 chunks Ø§Ø² 500 Ú©Ù„Ù…Ù‡
    
    document_id = uuid.uuid4()  # Group all chunks
    
    for i, chunk_text in enumerate(chunks):
        tldr = _extract_tldr(chunk_text, max_words=100)
        tldr_embedding = embedding_service.get_embedding(tldr)
        full_embedding = embedding_service.get_embedding(chunk_text)
        
        TenantKnowledge.objects.create(
            user=user,
            chunk_type='manual',
            document_id=document_id,  # Ù‡Ù…Ù‡ chunks ÛŒÚ© document_id Ø¯Ø§Ø±Ù†Ø¯
            section_title=f"Manual Prompt - Part {i+1}",
            full_text=chunk_text,
            tldr=tldr,
            tldr_embedding=tldr_embedding,
            full_embedding=full_embedding,
            word_count=len(chunk_text.split())
        )
```

### Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Chunking

**Ù‚ÙˆØ§Ù†ÛŒÙ†:**

```python
CHUNKING_RULES = {
    'faq': {
        'max_chunk_size': 'N/A',  # Ù‡Ø± QAPair = 1 chunk
        'tldr_size': 100,         # Ú©Ù„Ù…Ù‡
        'strategy': 'one_per_item'
    },
    'product': {
        'max_chunk_size': 'N/A',  # Ù‡Ø± Product = 1 chunk
        'tldr_size': 80,
        'strategy': 'one_per_item'
    },
    'manual': {
        'max_chunk_size': 500,    # Ú©Ù„Ù…Ù‡
        'tldr_size': 100,
        'strategy': 'hierarchical_split',
        'preserve': 'paragraphs'  # Ø­ÙØ¸ Ù…Ø±Ø² Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§
    },
    'website': {
        'max_chunk_size': 500,    # Ú©Ù„Ù…Ù‡
        'tldr_size': 100,
        'strategy': 'hierarchical_split',
        'preserve': 'paragraphs'
    }
}
```

**Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Text Splitting:**

```python
def _chunk_text(text, max_words=500):
    """
    Smart chunking Ú©Ù‡ Ù…Ø±Ø² Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ Ø±Ùˆ Ø­ÙØ¸ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
    """
    if len(text.split()) <= max_words:
        return [text]
    
    chunks = []
    paragraphs = text.split('\n\n')
    
    current_chunk = []
    current_words = 0
    
    for para in paragraphs:
        para_words = len(para.split())
        
        if current_words + para_words <= max_words:
            current_chunk.append(para)
            current_words += para_words
        else:
            if current_chunk:
                chunks.append('\n\n'.join(current_chunk))
            
            # Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ØŸ Ø¨Ø´Ú©Ù† Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª
            if para_words > max_words:
                sentences = para.split('. ')
                # ... (split by sentences)
            else:
                current_chunk = [para]
                current_words = para_words
    
    if current_chunk:
        chunks.append('\n\n'.join(current_chunk))
    
    return chunks
```

---

## ğŸŒ Knowledge Sources Ùˆ Crawling

### 4 Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ (Knowledge Sources)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Knowledge Sources                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. FAQ (QAPair)                    3. Products
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Source: web_knowledge.QAPair       Source: web_knowledge.Product
Trigger: post_save signal          Trigger: post_save signal
Chunking: 1 QAPair â†’ 1 chunk      Chunking: 1 Product â†’ 1 chunk
Format:                            Format:
  Q: {question}                      **{title}**
  A: {answer}                        {description}
                                     Price: {price}
                                     Link: {link}

2. Manual Prompt                   4. Website Pages
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Source: settings.AIPrompts         Source: web_knowledge.WebsitePage
Trigger: post_save signal          Trigger: Async crawl task
Chunking: Split every 500 words    Chunking: Split every 500 words
Strategy: Hierarchical             Strategy: Hierarchical
```

### Website Crawler (Ø§ØªÙˆÙ…Ø§ØªÛŒÚ©)

**Ú©Ø¯:** `src/web_knowledge/services/crawler_service.py`

**Ù…Ø¹Ù…Ø§Ø±ÛŒ:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Website Crawler Flow                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User â†’ Create WebsiteSource
â”‚      â”œâ”€ URL: https://example.com
â”‚      â”œâ”€ max_pages: 30
â”‚      â”œâ”€ crawl_depth: 3
â”‚      â””â”€ auto_extract_products: true
â”‚
â”œâ”€â†’ POST /api/v1/web-knowledge/websites/create-and-crawl/
â”‚
â””â”€â†’ Celery Task: crawl_website_task.delay(website_id)
     â”‚
     â”œâ”€â†’ 1. Initialize WebsiteCrawler
     â”‚    â”œâ”€ Respectful delay: 2.0s between requests
     â”‚    â”œâ”€ User-Agent: "Fiko WebKnowledge Bot 1.0"
     â”‚    â””â”€ Connection pooling (Session)
     â”‚
     â”œâ”€â†’ 2. BFS Crawling
     â”‚    â”œâ”€ Start: base_url (depth=0)
     â”‚    â”œâ”€ Extract links from page
     â”‚    â”œâ”€ Filter: same domain (unless include_external=true)
     â”‚    â”œâ”€ Queue new URLs (depth+1)
     â”‚    â””â”€ Stop: max_pages OR max_depth
     â”‚
     â”œâ”€â†’ 3. Extract Content (per page)
     â”‚    â”œâ”€ BeautifulSoup HTML parsing
     â”‚    â”œâ”€ Remove: <script>, <style>, <nav>, <footer>
     â”‚    â”œâ”€ Clean: whitespace, ads, menus
     â”‚    â””â”€ Save: WebsitePage.cleaned_content
     â”‚
     â”œâ”€â†’ 4. Generate Q&A Pairs (per page)
     â”‚    â”œâ”€ Check: word_count >= 100
     â”‚    â”œâ”€ AI: QAGenerator.generate_qa_pairs()
     â”‚    â”‚   â”œâ”€ Model: Gemini 1.5 Flash
     â”‚    â”‚   â”œâ”€ Prompt: "Generate FAQ from this page..."
     â”‚    â”‚   â””â”€ Output: JSON [{Q, A}, ...]
     â”‚    â”œâ”€ Validation: Remove bad Q&A
     â”‚    â””â”€ Save: QAPair (auto-linked to page)
     â”‚
     â”œâ”€â†’ 5. Auto-Extract Products (if enabled)
     â”‚    â”œâ”€ Pre-filter: Page has product keywords?
     â”‚    â”‚   (Ù‚ÛŒÙ…Øª, price, Ø®Ø±ÛŒØ¯, buy, Ù…Ø­ØµÙˆÙ„, product)
     â”‚    â”œâ”€ AI: Gemini Pro "Extract products from page..."
     â”‚    â””â”€ Save: Product (auto-linked to page)
     â”‚
     â””â”€â†’ 6. Chunk Pages
          â””â”€â†’ chunk_webpage_async.delay(page_id)
               â””â”€â†’ TenantKnowledge chunks (1-5 per page)
```

**Ù…Ø«Ø§Ù„ Crawler Code:**

```python
class WebsiteCrawler:
    def __init__(self, base_url, max_pages=30, max_depth=3, delay=2.0):
        self.base_url = base_url
        self.max_pages = max_pages
        self.max_depth = max_depth
        self.delay = delay  # Respectful crawling
        
        self.visited_urls = set()
        self.crawled_pages = []
        
        # Session for connection reuse
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Fiko WebKnowledge Bot 1.0'
        })
    
    def crawl(self):
        urls_to_crawl = [(self.base_url, 0)]  # (url, depth)
        
        while urls_to_crawl and len(self.crawled_pages) < self.max_pages:
            current_url, depth = urls_to_crawl.pop(0)
            
            if current_url in self.visited_urls or depth > self.max_depth:
                continue
            
            # Crawl page
            page_data = self._crawl_page(current_url, depth)
            if page_data:
                self.crawled_pages.append(page_data)
                
                # Extract new URLs
                new_urls = self._extract_urls(page_data['links'], depth + 1)
                urls_to_crawl.extend(new_urls)
            
            # Respectful delay
            time.sleep(self.delay)
        
        return self.crawled_pages
    
    def _crawl_page(self, url, depth):
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            # Parse HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extract content
            title = soup.find('title').text if soup.find('title') else url
            
            # Remove unwanted elements
            for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
                tag.decompose()
            
            # Get text content
            raw_content = soup.get_text(separator='\n')
            cleaned_content = self._clean_text(raw_content)
            
            # Extract links
            links = [a['href'] for a in soup.find_all('a', href=True)]
            
            self.visited_urls.add(url)
            
            return {
                'url': url,
                'title': title,
                'raw_content': raw_content,
                'cleaned_content': cleaned_content,
                'links': links,
                'depth': depth,
                'word_count': len(cleaned_content.split())
            }
            
        except Exception as e:
            logger.error(f"Failed to crawl {url}: {e}")
            return None
```

### Q&A Auto-Generation

**Ú©Ø¯:** `src/web_knowledge/services/qa_generator.py`

```python
class QAGenerator:
    def generate_qa_pairs(self, content, page_title, max_pairs=5):
        """
        ØªÙˆÙ„ÛŒØ¯ Ø§ØªÙˆÙ…Ø§ØªÛŒÚ© Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨ Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ù‡
        """
        # Split content if too long
        content_chunks = self._split_content(content, max_chunk_size=3000)
        
        all_qa_pairs = []
        for chunk in content_chunks:
            prompt = f"""
You are an expert creating natural Q&A pairs from website content.

Page: {page_title}
Content: {chunk}

RULES:
- Create {max_pairs} natural questions as if a customer is asking
- Provide COMPLETE, SPECIFIC answers with actual details
- Use real info: prices, contact, features, policies
- NO generic templates
- NO URLs in questions

Format as JSON:
[
  {{"question": "...", "answer": "..."}},
  ...
]
"""
            
            response = self.model.generate_content(prompt)
            qa_json = self._extract_json(response.text)
            all_qa_pairs.extend(qa_json)
        
        # Validate & clean
        validated = self._validate_qa_pairs(all_qa_pairs)
        return validated[:max_pairs]
```

---

## ğŸ” Query Router (Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ÛŒ Ø³ÙˆØ§Ù„)

**Ú©Ø¯:** `src/AI_model/services/query_router.py`

### ØªØ´Ø®ÛŒØµ Intent

**Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶:**

```python
DEFAULT_KEYWORDS = {
    'pricing': {
        'fa': ['Ù‚ÛŒÙ…Øª', 'Ù‡Ø²ÛŒÙ†Ù‡', 'ØªØ¹Ø±ÙÙ‡', 'Ù¾Ù„Ù†', 'Ù¾Ú©ÛŒØ¬', 'Ø§Ø´ØªØ±Ø§Ú©', 'Ø®Ø±ÛŒØ¯'],
        'en': ['price', 'cost', 'plan', 'subscription', 'buy', 'payment'],
        'ar': ['Ø³Ø¹Ø±', 'ØªÙƒÙ„ÙØ©', 'Ø®Ø·Ø©', 'Ø§Ø´ØªØ±Ø§Ùƒ', 'Ø´Ø±Ø§Ø¡'],
        'tr': ['fiyat', 'maliyet', 'plan', 'abonelik']
    },
    'product': {
        'fa': ['Ù…Ø­ØµÙˆÙ„', 'Ø³Ø±ÙˆÛŒØ³', 'Ø®Ø¯Ù…Ø§Øª', 'ÙˆÛŒÚ˜Ú¯ÛŒ', 'Ø§Ù…Ú©Ø§Ù†Ø§Øª', 'Ú†ÛŒÙ‡'],
        'en': ['product', 'service', 'feature', 'what is'],
        'ar': ['Ù…Ù†ØªØ¬', 'Ø®Ø¯Ù…Ø©', 'Ù…ÙŠØ²Ø©'],
        'tr': ['Ã¼rÃ¼n', 'hizmet', 'Ã¶zellik']
    },
    'howto': {
        'fa': ['Ú†Ø·ÙˆØ±', 'Ú†Ú¯ÙˆÙ†Ù‡', 'Ø±Ø§Ù‡Ù†Ù…Ø§', 'Ø¢Ù…ÙˆØ²Ø´', 'Ù†Ø­ÙˆÙ‡', 'Ú©Ù…Ú©'],
        'en': ['how', 'guide', 'tutorial', 'help'],
        'ar': ['ÙƒÙŠÙ', 'Ø¯Ù„ÙŠÙ„', 'Ù…Ø³Ø§Ø¹Ø¯Ø©'],
        'tr': ['nasÄ±l', 'rehber', 'yardÄ±m']
    },
    'contact': {
        'fa': ['ØªÙ…Ø§Ø³', 'Ø§Ø±ØªØ¨Ø§Ø·', 'Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ', 'Ø´Ù…Ø§Ø±Ù‡', 'Ø§ÛŒÙ…ÛŒÙ„', 'Ø¢Ø¯Ø±Ø³'],
        'en': ['contact', 'support', 'phone', 'email'],
        'ar': ['Ø§ØªØµØ§Ù„', 'Ø¯Ø¹Ù…', 'Ù‡Ø§ØªÙ'],
        'tr': ['iletiÅŸim', 'destek', 'telefon']
    }
}
```

### ØªÙ†Ø¸ÛŒÙ…Ø§Øª Routing

```python
DEFAULT_ROUTING = {
    'pricing': {
        'primary_source': 'faq',
        'secondary_sources': ['products', 'manual'],
        'token_budget': {'primary': 800, 'secondary': 300}
    },
    'product': {
        'primary_source': 'products',
        'secondary_sources': ['faq', 'website'],
        'token_budget': {'primary': 800, 'secondary': 300}
    },
    'howto': {
        'primary_source': 'manual',
        'secondary_sources': ['faq', 'website'],
        'token_budget': {'primary': 800, 'secondary': 300}
    },
    'contact': {
        'primary_source': 'manual',
        'secondary_sources': ['website'],
        'token_budget': {'primary': 800, 'secondary': 300}
    },
    'general': {
        'primary_source': 'faq',
        'secondary_sources': ['manual'],
        'token_budget': {'primary': 800, 'secondary': 300}
    }
}
```

---

## ğŸ“Š Context Retriever (RAG Ø¨Ø§ pgvector)

### Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ

**SQL Query:**

```sql
-- Top-K Retrieval Ø¨Ø§ Cosine Similarity
SELECT 
    id,
    section_title,
    full_text,
    chunk_type,
    word_count,
    (1 - (tldr_embedding <=> $1::vector(1536))) AS similarity
FROM 
    ai_model_tenantknowledge
WHERE 
    user_id = $2
    AND chunk_type = $3
    AND tldr_embedding IS NOT NULL
    AND (1 - (tldr_embedding <=> $1::vector(1536))) >= 0.1  -- MIN_SIMILARITY
ORDER BY 
    tldr_embedding <=> $1::vector(1536)  -- Cosine distance
LIMIT 10;
```

### Index Ø¨Ø±Ø§ÛŒ Performance

```sql
-- IVFFlat Index (Approximate Nearest Neighbor)
CREATE INDEX idx_tenant_knowledge_tldr_embedding 
ON ai_model_tenantknowledge 
USING ivfflat (tldr_embedding vector_cosine_ops)
WITH (lists = 100);

-- Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ:
-- lists = sqrt(rows) â†’ Ø¨Ø±Ø§ÛŒ 10000 rowØŒ lists = 100
-- Tradeoff: Speed vs Accuracy
```

---

## ğŸ’° Token Management

### Ù…Ø¯ÛŒØ±ÛŒØª ØªÙˆÚ©Ù†

```python
# Pre-check (Ù‚Ø¨Ù„ Ø§Ø² AI call)
estimated_tokens = 700  # Ø¨Ø±Ø§ÛŒ prompt enhancement
if subscription.tokens_remaining < estimated_tokens:
    raise Exception('Insufficient tokens')

# Consume (Ø¨Ø¹Ø¯ Ø§Ø² AI call)
actual_tokens = response.usage_metadata.total_token_count  # Ù…Ø«Ù„Ø§Ù‹ 1389
consume_tokens_for_user(user, actual_tokens, description='AI response')

# Update subscription
subscription.tokens_used += actual_tokens
subscription.tokens_remaining -= actual_tokens
subscription.save()
```

---

## ğŸš€ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ

### Ù…Ø«Ø§Ù„ 1: Ø³ÙˆØ§Ù„ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù‚ÛŒÙ…Øª

**Input:**
```
User: "Ù‚ÛŒÙ…Øª Ù¾Ù„Ù† Pro Ú†Ù†Ø¯Ù‡ØŸ"
```

**Processing:**

```python
# 1. Query Router
routing = {
    'intent': 'pricing',
    'confidence': 0.92,
    'primary_source': 'faq',
    'secondary_sources': ['products'],
    'keywords_matched': ['Ù‚ÛŒÙ…Øª', 'Ù¾Ù„Ù†']
}

# 2. Embedding
query_embedding = [0.0234, -0.1456, 0.0892, ..., 0.0234]  # 1536 dims

# 3. Context Retrieval (pgvector)
primary_chunks = [
    {
        'title': 'Ù‚ÛŒÙ…Øª Ù¾Ù„Ù†â€ŒÙ‡Ø§ÛŒ Ø§Ø´ØªØ±Ø§Ú©ÛŒ',
        'content': 'Ù…Ø§ 3 Ù¾Ù„Ù† Ø¯Ø§Ø±ÛŒÙ…...',
        'score': 0.892
    }
]
secondary_chunks = [
    {
        'title': 'Ù¾Ù„Ù† Professional',
        'content': 'Ù¾Ù„Ù† Pro - $29/Ù…Ø§Ù‡...',
        'score': 0.765
    }
]

# 4. Prompt Building
prompt = """
SYSTEM: {combined_prompt}
KNOWLEDGE BASE: {primary_chunks + secondary_chunks}
QUESTION: Ù‚ÛŒÙ…Øª Ù¾Ù„Ù† Pro Ú†Ù†Ø¯Ù‡ØŸ
"""
# Total: 1180 tokens

# 5. Gemini Response
ai_response = "Ù¾Ù„Ù† Professional (Pro) Ù…Ø§ Ø¨Ù‡ Ù‚ÛŒÙ…Øª $29 Ø¯Ø± Ù…Ø§Ù‡ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒØ´Ù‡ Ú©Ù‡ Ø´Ø§Ù…Ù„..."
tokens_used = 1389  # 1180 input + 209 output
```

**Output:**
```
AI: Ù¾Ù„Ù† Professional (Pro) Ù…Ø§ Ø¨Ù‡ Ù‚ÛŒÙ…Øª $29 Ø¯Ø± Ù…Ø§Ù‡ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒØ´Ù‡ Ú©Ù‡ Ø´Ø§Ù…Ù„:

âœ… 5000 ØªÙˆÚ©Ù† AI Ù…Ø§Ù‡Ø§Ù†Ù‡
âœ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¯Ø§Ø± 24/7
âœ… API access
âœ… Advanced analytics
âœ… Ù†ØµØ¨ Ø±ÙˆÛŒ Ø¯Ø§Ù…Ù†Ù‡ Ø´Ø®ØµÛŒ

Ø§ÛŒÙ† Ù¾Ù„Ù† Ø¨Ø±Ø§ÛŒ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø±Ù‡Ø§ÛŒ Ù…ØªÙˆØ³Ø· Ù…Ù†Ø§Ø³Ø¨Ù‡! ğŸ˜Š
```

---

### Ù…Ø«Ø§Ù„ 2: Ø³ÙˆØ§Ù„ Ø®Ø§Ø±Ø¬ Ø§Ø² Ø­ÙˆØ²Ù‡

**Input:**
```
User: "Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§ÛŒ ØªÙ‡Ø±Ø§Ù† Ø§Ù…Ø±ÙˆØ² Ú†Ø·ÙˆØ±Ù‡ØŸ"
```

**Processing:**

```python
# 1. Query Router
routing = {
    'intent': 'general',  # Ù‡ÛŒÚ† keyword Ù…Ø±ØªØ¨Ø· Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯
    'confidence': 0.5,
    'primary_source': 'faq',
    'secondary_sources': ['manual']
}

# 2. Context Retrieval
# Ù‡ÛŒÚ† chunk Ù…Ø±ØªØ¨Ø· Ø¨Ø§ score > 0.1 Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯
primary_chunks = []
secondary_chunks = []

# 3. Confidence Instruction
# Low confidence â†’ Tell user we can't help with this
```

**Output:**
```
AI: Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ù…Ù† Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ù†Ø¯Ø§Ø±Ù…. Ù…Ù† ÙÙ‚Ø· Ù…ÛŒØªÙˆÙ†Ù… Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª Ùˆ Ø®Ø¯Ù…Ø§Øª Ù…Ø§ØŒ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ØŒ Ùˆ Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù…Ú©ØªÙˆÙ† Ú©Ù†Ù…. ğŸ˜Š

Ø¢ÛŒØ§ Ø³ÙˆØ§Ù„ Ø¯ÛŒÚ¯Ù‡â€ŒØ§ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ù…Ø§ Ø¯Ø§Ø±ÛŒØ¯ØŸ
```

---

## ğŸ“ˆ Monitoring Ùˆ Performance

### Metrics

```python
# AI Usage Tracking (per day, per user)
class AIUsageTracking:
    user = ForeignKey(User)
    date = DateField()
    
    total_requests = IntegerField()          # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
    successful_requests = IntegerField()     # Ù…ÙˆÙÙ‚
    failed_requests = IntegerField()         # Ù†Ø§Ù…ÙˆÙÙ‚
    
    total_prompt_tokens = IntegerField()     # ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ
    total_completion_tokens = IntegerField() # ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
    total_tokens = IntegerField()            # Ù…Ø¬Ù…ÙˆØ¹
    
    total_response_time_ms = IntegerField()  # Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø® (ms)
    avg_response_time_ms = FloatField()      # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†
```

### Performance Targets

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Performance Targets (v2.1)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Metric                      Target        Current    Status
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input Tokens per Query      â‰¤ 1500        ~1200      âœ…
Output Tokens per Response  â‰¤ 500         ~200       âœ…
Total Response Time         â‰¤ 3s          ~2.3s      âœ…
RAG Retrieval Time          â‰¤ 100ms       ~45ms      âœ…
Embedding Cache Hit Rate    â‰¥ 80%         ~85%       âœ…
Answer Accuracy             â‰¥ 90%         ~92%       âœ…
Multilingual Support        4 langs       4 langs    âœ…
  (FA, EN, AR, TR)
```

---

## ğŸ“ Ø®Ù„Ø§ØµÙ‡ ÙÙ†ÛŒ

### Stack Technology

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Technology Stack                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Backend:
  - Django 4.2+
  - PostgreSQL 15+ with pgvector
  - Redis (Cache)
  - Celery (Async tasks)

AI Services:
  - Google Gemini 2.5 Flash (Response generation)
  - OpenAI text-embedding-3-small (Primary embedding)
  - Google Gemini text-embedding-004 (Fallback embedding)

Vector Database:
  - pgvector extension
  - IVFFlat index (Approximate Nearest Neighbor)
  - Cosine similarity search

Libraries:
  - google-generativeai (Gemini API)
  - openai (OpenAI API)
  - pgvector (Django integration)
  - beautifulsoup4 (Web scraping)
  - rank-bm25 (BM25 fallback)
```

### Key Features

âœ… **Lean RAG Architecture** - Ù‡Ø²ÛŒÙ†Ù‡ ØªÙˆÚ©Ù† Ú©Ù…ØªØ± (â‰¤1500)  
âœ… **Multi-Source Retrieval** - 4 Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ù†Ø´ (FAQ, Manual, Products, Website)  
âœ… **Semantic Search** - Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨Ø§ pgvector  
âœ… **Multilingual** - Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² 4 Ø²Ø¨Ø§Ù† (FA/EN/AR/TR)  
âœ… **Real-Time Chunking** - Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø§ØªÙˆÙ…Ø§ØªÛŒÚ© Ø¨Ø§ Django Signals  
âœ… **Smart Routing** - Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ keyword matching  
âœ… **Token Management** - Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ù‚ÛŒÙ‚ Ù…ØµØ±Ù ØªÙˆÚ©Ù†  
âœ… **Conversation Memory** - Ø­Ø§ÙØ¸Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¨Ø§ rolling summary  
âœ… **Auto-Crawling** - Ú©Ø±Ø§Ù„ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§ØªÙˆÙ…Ø§ØªÛŒÚ© Ù…Ø­ØªÙˆØ§  

---

## ğŸ“ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ

Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª ÙÙ†ÛŒ ÛŒØ§ Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø§Ú¯ØŒ Ù„Ø·ÙØ§Ù‹ Ø¨Ù‡ ØªÛŒÙ… AI Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯.

**Repository:** `https://github.com/fiko/backend`  
**Documentation:** Ø§ÛŒÙ† ÙØ§ÛŒÙ„ + Ú©Ø¯Ù‡Ø§ÛŒ Ù…Ù†Ø¨Ø¹  
**Version:** 2.1 (October 2025)

---

**ğŸ‰ Ù¾Ø§ÛŒØ§Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª**

Ø§ÛŒÙ† Ù…Ø³ØªÙ†Ø¯ Ø´Ø§Ù…Ù„ ØªÙ…Ø§Ù…ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙÙ†ÛŒØŒ Ù…Ø¹Ù…Ø§Ø±ÛŒØŒ Ùˆ Ù†Ø­ÙˆÙ‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ… Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ FIKO Ø§Ø³Øª.

