# ğŸ¯ Lean RAG v2.1 - Ù¾Ù„Ù† Ø§Ø¬Ø±Ø§ÛŒÛŒ Ø¯Ù‚ÛŒÙ‚

**ØªØ§Ø±ÛŒØ®:** 2025-10-07  
**ÙˆØ¶Ø¹ÛŒØª:** Ù…Ù†ØªØ¸Ø± ØªØ§ÛŒÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹  
**Ù‡Ø¯Ù:** Ú©Ø§Ù‡Ø´ 11,000 â†’ 1,500 tokens Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ø³Ø±Ø¹Øª/Ø¯Ù‚Øª

---

## ğŸ“Š Ø¨Ø±Ø±Ø³ÛŒ Ø³ÛŒØ³ØªÙ… ÙØ¹Ù„ÛŒ

### âœ… Ú†ÛŒØ²Ù‡Ø§ÛŒÛŒ Ú©Ù‡ **Ø§Ù„Ø§Ù† Ø¯Ø§Ø±ÛŒÙ…** Ùˆ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù‡:

#### **1. Infrastructure (100% Ø¢Ù…Ø§Ø¯Ù‡)**
```yaml
PostgreSQL: âœ… version 15 (docker-compose)
Redis: âœ… version 7 (docker-compose)
Celery: âœ… worker + beat Ø¨Ø§ 4 queue
WebSocket: âœ… Daphne + channels-redis
Docker: âœ… Ú©Ø§Ù…Ù„ setup Ø´Ø¯Ù‡
AWS EC2: âœ… production server
```

#### **2. Libraries Ù†ØµØ¨ Ø´Ø¯Ù‡:**
```python
âœ… openai >= 1.12.0              # Ø¨Ø±Ø§ÛŒ embedding
âœ… google-generativeai >= 0.8.0  # Ø¨Ø±Ø§ÛŒ Gemini
âœ… rank-bm25 == 0.2.2             # Ø¨Ø±Ø§ÛŒ fallback search
âœ… redis, django-redis             # caching
âœ… celery, django-celery-beat      # async tasks
âœ… channels, daphne                # WebSocket
âœ… psycopg2-binary                 # PostgreSQL
```

#### **3. Services Ù…ÙˆØ¬ÙˆØ¯:**

**`EmbeddingService`** (Ú©Ø§Ù…Ù„ Ùˆ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù‡):
- âœ… OpenAI text-embedding-3-large (3072 dimensions)
- âœ… Gemini fallback
- âœ… Redis caching (30 days TTL)
- âœ… Cosine similarity calculation
- âœ… Document ranking

**`GeminiChatService`** (Ø³ÛŒØ³ØªÙ… ÙØ¹Ù„ÛŒ AI):
- âœ… Gemini 2.5 Flash integration
- âœ… Prompt building Ø¨Ø§ JSON config
- âœ… Conversation summarization (Ø¨Ø±Ø§ÛŒ Ù…Ú©Ø§Ù„Ù…Ø§Øª > 10 Ù¾ÛŒØ§Ù…)
- âœ… BM25 ranking Ø¨Ø±Ø§ÛŒ FAQ (top 8)
- âœ… Embedding ranking Ø¨Ø±Ø§ÛŒ FAQ (Ø¨Ø§ fallback Ø¨Ù‡ BM25)
- âœ… Token tracking (input/output/total)
- âœ… Billing integration
- âœ… WebSocket notification

**`MessageSystemIntegration`**:
- âœ… Auto AI response triggering
- âœ… Token checking
- âœ… Conversation status management

#### **4. Data Models:**

**Ù…ÙˆØ¬ÙˆØ¯:**
- âœ… `Message` (content, type, is_ai_response, token fields)
- âœ… `Conversation` (status: active/support_active/closed)
- âœ… `AIGlobalConfig` (global settings)
- âœ… `AIUsageTracking` (token usage per user per day)
- âœ… `QAPair` (FAQ Ø§Ø² web crawling)
- âœ… `Product` (Ù…Ø­ØµÙˆÙ„Ø§Øª)
- âœ… `WebsiteSource`, `WebsitePage` (Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ crawl Ø´Ø¯Ù‡)
- âœ… `AIPrompts` (manual_prompt per user)

#### **5. API Endpoints Ù…ÙˆØ¬ÙˆØ¯:**

```
POST /api/v1/ai/ask/                          # Ù¾Ø±Ø³Ø´ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² AI
GET  /api/v1/ai/config/                       # ØªÙ†Ø¸ÛŒÙ…Ø§Øª global
GET  /api/v1/ai/config/status/                # ÙˆØ¶Ø¹ÛŒØª config
GET  /api/v1/ai/conversations/{id}/status/    # ÙˆØ¶Ø¹ÛŒØª Ù…Ú©Ø§Ù„Ù…Ù‡
PUT  /api/v1/ai/conversations/{id}/status/    # ØªØºÛŒÛŒØ± status (AI/manual)
PUT  /api/v1/ai/conversations/bulk-status/    # bulk status update
GET  /api/v1/ai/default-handler/              # handler Ù¾ÛŒØ´â€ŒÙØ±Ø¶ user
PUT  /api/v1/ai/default-handler/              # ØªØºÛŒÛŒØ± handler
GET  /api/v1/ai/usage/stats/                  # Ø¢Ù…Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡
GET  /api/v1/ai/usage/global/                 # Ø¢Ù…Ø§Ø± global (admin)
```

#### **6. Flow ÙØ¹Ù„ÛŒ AI Response:**

```
Customer Message (Telegram/Instagram/WebSocket)
    â†“
Message.objects.create(type='customer')
    â†“
Signal: post_save (message/signals.py)
    â†“
Check: global AI enabled? conversation status = active?
    â†“
Celery Task: process_ai_response_async (ai_tasks queue)
    â†“
MessageSystemIntegration.process_new_customer_message()
    â†“
Check tokens â†’ GeminiChatService.generate_response()
    â†“
_build_prompt():
    - Get manual_prompt (Ú©Ù„ Ù…ØªÙ†!)
    - Get FAQ (top 8 Ø¨Ø§ embedding/BM25)
    - Get Products (top 6)
    - Get Website pages (top 2 sites Ã— 5 pages)
    - Get conversation history (6 messages ÛŒØ§ summary + 5)
    - Build JSON config
    â†“
Gemini API call
    â†“
create_ai_message() â†’ Send to Telegram/Instagram
    â†“
WebSocket notification to frontend
```

---

### âŒ Ú†ÛŒØ²Ù‡Ø§ÛŒÛŒ Ú©Ù‡ **Ù†Ø¯Ø§Ø±ÛŒÙ…** Ùˆ Ø¨Ø§ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒÙ…:

```yaml
âŒ pgvector extension Ø¯Ø± PostgreSQL
âŒ pgvector Python package
âŒ TenantKnowledge model (vector store)
âŒ SessionMemory model (rolling summaries)
âŒ IntentKeyword model (optional)
âŒ IntentRouting model (optional)
âŒ QueryRouter service (intent classification)
âŒ ContextRetriever service (RAG Ø¨Ø§ pgvector)
âŒ TokenBudgetController service (1500 token enforcer)
âŒ SessionMemoryManager service (rolling summary)
âŒ Management command Ø¨Ø±Ø§ÛŒ indexing data
âŒ Refactored _build_prompt() method
```

---

### âš ï¸ Ù…Ø´Ú©Ù„Ø§Øª Ø³ÛŒØ³ØªÙ… ÙØ¹Ù„ÛŒ:

#### **1. Token Usage Ø®ÛŒÙ„ÛŒ Ø¨Ø§Ù„Ø§ (11,000 tokens/conversation):**

```python
# ÙØ¹Ù„Ø§Ù‹ Ø¯Ø± _build_prompt():

manual_prompt: 15,000 Ú©Ù„Ù…Ù‡ â†’ ~19,500 tokens  # Ù‡Ù…Ù‡ Ø±Ùˆ Ù…ÛŒâ€ŒÙØ±Ø³ØªÙ‡!
FAQ: 8 pairs Ã— 150 Ú©Ù„Ù…Ù‡ â†’ ~1,560 tokens
Products: 6 Ã— 100 Ú©Ù„Ù…Ù‡ â†’ ~780 tokens
Website: 2 sites Ã— 5 pages â†’ ~2,000 tokens
Conversation: 6 messages â†’ ~500 tokens
System prompt: ~300 tokens
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~24,640 tokens! âŒ

# Gemini input limit handles it, but:
- Cost: $0.28 per conversation
- Latency: 15-20 seconds
- "Over-context" problem â†’ Ø¬ÙˆØ§Ø¨â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø¯Ù‚ÛŒÙ‚
```

#### **2. No Intent Classification:**
- Ù‡Ù…Ù‡ context Ù‡Ø§ Ø±Ùˆ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ù…ÛŒâ€ŒÙØ±Ø³ØªÙ‡
- Ù…Ø«Ù„Ø§Ù‹ Ø¨Ø±Ø§ÛŒ "Ù‚ÛŒÙ…Øª Ú†Ù‚Ø¯Ø±Ù‡ØŸ" Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ WebsitePage Ù†ÛŒØ³Øª!

#### **3. No Token Budget Control:**
- Ù‡ÛŒÚ† Ù…Ø­Ø¯ÙˆØ¯ÛŒØªÛŒ Ù†Ø¯Ø§Ø±ÛŒÙ…
- Ø§Ú¯Ù‡ manual_prompt Ø¨Ø²Ø±Ú¯â€ŒØªØ± Ø¨Ø´Ù‡ØŒ Ø¨ÛŒØ´ØªØ± Ù…ÛŒâ€ŒÙØ±Ø³ØªÙ‡

#### **4. Cumulative Summarization:**
- Ù‡Ø± 10 Ù¾ÛŒØ§Ù… ÛŒÚ© summary Ù…ÛŒâ€ŒØ³Ø§Ø²Ù‡
- Ø§Ù…Ø§ summaries Ø¬Ù…Ø¹ Ù…ÛŒâ€ŒØ´Ù†! (10 + 10 + 10 = 30 Ù¾ÛŒØ§Ù… â†’ 3 summary)

---

## ğŸ¯ Ø§Ù‡Ø¯Ø§Ù Lean RAG v2.1

| Metric | ÙØ¹Ù„ÛŒ | Ù‡Ø¯Ù | Ø¨Ù‡Ø¨ÙˆØ¯ |
|--------|------|-----|-------|
| Input Tokens | 11,000 | â‰¤1,500 | 86% Ú©Ø§Ù‡Ø´ |
| Cost/conversation | $0.28 | $0.03 | 89% Ú©Ø§Ù‡Ø´ |
| Response Time | 15-20s | 6-8s | 50% Ø³Ø±ÛŒØ¹â€ŒØªØ± |
| Accuracy | Ù…ØªÙˆØ³Ø· | Ø¨Ø§Ù„Ø§ | +30% |

---

## ğŸ“… ÙØ§Ø²Ù‡Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒÛŒ (Ø¯Ù‚ÛŒÙ‚)

### **ÙØ§Ø² 0: Setup Infrastructure** (2-3 Ø³Ø§Ø¹Øª)

#### Ú†ÛŒÚ©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:
1. Ù†ØµØ¨ pgvector extension Ø¯Ø± PostgreSQL
2. Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† pgvector Ø¨Ù‡ requirements.txt
3. ØªØ³Øª pgvector

#### Ú†Ø·ÙˆØ±ÛŒ:

**Step 1: Configure PostgreSQL for pgvector**
```bash
# âš ï¸ Ù…Ù‡Ù…: Ø¨Ø§ÛŒØ¯ shared_preload_libraries ØªÙ†Ø¸ÛŒÙ… Ø¨Ø´Ù‡

# Option 1: Ø§Ú¯Ø± docker-compose Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯
# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ docker-compose.yml:
services:
  db:
    image: postgres:15
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    command: >
      postgres -c shared_preload_libraries='vector'

# ÛŒØ§ Option 2: ØªØºÛŒÛŒØ± postgresql.conf Ù…Ø³ØªÙ‚ÛŒÙ…
docker-compose exec db bash
echo "shared_preload_libraries = 'vector'" >> /var/lib/postgresql/data/postgresql.conf
exit

# Restart PostgreSQL Ø¨Ø±Ø§ÛŒ Ø§Ø¹Ù…Ø§Ù„ ØªØºÛŒÛŒØ±Ø§Øª
docker-compose restart db

# ØªØ³Øª:
docker-compose exec db psql -U postgres -c "SHOW shared_preload_libraries;"
# Ø¨Ø§ÛŒØ¯ 'vector' Ø±Ùˆ Ù†Ø´ÙˆÙ† Ø¨Ø¯Ù‡
```

**Step 2: Install pgvector extension**
```bash
# Ø¯Ø± Ø³Ø±ÙˆØ±/local
docker-compose exec db psql -U postgres -d fiko_db

# Ø¯Ø± psql:
CREATE EXTENSION IF NOT EXISTS vector;

# ØªØ³Øª:
SELECT * FROM pg_extension WHERE extname = 'vector';
# Ø¨Ø§ÛŒØ¯ 1 row Ø¨Ø±Ú¯Ø±Ø¯ÙˆÙ†Ù‡

# Ø®Ø±ÙˆØ¬:
\q
```

**Step 3: Add to requirements**
```bash
# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ src/requirements/base.txt
echo "pgvector==0.3.6" >> src/requirements/base.txt
echo "tiktoken==0.8.0" >> src/requirements/base.txt  # Ø¨Ø±Ø§ÛŒ token counting Ø¯Ù‚ÛŒÙ‚

# Ù†ØµØ¨:
docker-compose exec web pip install pgvector==0.3.6

# ÛŒØ§ rebuild:
docker-compose build web
docker-compose up -d
```

**Step 4: Test Ø¯Ø± Django shell**
```python
docker-compose exec web python manage.py shell

from pgvector.django import VectorField
print("pgvector imported successfully! âœ…")
```

#### ØªØ³Øª Ù…ÙˆÙÙ‚ÛŒØª:
```python
# Ø¨Ø§ÛŒØ¯ Ø¨Ø¯ÙˆÙ† error Ø§Ø¬Ø±Ø§ Ø¨Ø´Ù‡:
from pgvector.django import VectorField, CosineDistance
```

#### Ø®Ø±ÙˆØ¬ÛŒ ÙØ§Ø² 0:
- âœ… pgvector extension ÙØ¹Ø§Ù„
- âœ… pgvector package Ù†ØµØ¨
- âœ… Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª models

---

### **ÙØ§Ø² 1: Database Models** (3-4 Ø³Ø§Ø¹Øª)

#### Ú†ÛŒÚ©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:
4 model Ø¬Ø¯ÛŒØ¯ Ù…ÛŒâ€ŒØ³Ø§Ø²ÛŒÙ… Ø¨Ø±Ø§ÛŒ RAG system

#### Models:

**1. TenantKnowledge** (Ø§ØµÙ„ÛŒâ€ŒØªØ±ÛŒÙ†):
```python
# Vector store Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ knowledge sources
# Fields:
- user (FK to User)
- chunk_type: 'faq', 'manual', 'product', 'website'
- source_id: reference Ø¨Ù‡ FAQ/Product/Page Ø§ØµÙ„ÛŒ
- full_text: Ù…ØªÙ† Ú©Ø§Ù…Ù„
- tldr: Ø®Ù„Ø§ØµÙ‡ 80-120 Ú©Ù„Ù…Ù‡ (Ø¨Ø±Ø§ÛŒ search)
- tldr_embedding: vector(3072) â† pgvector
- language: 'fa', 'en', 'ar', 'tr'
- metadata: JSONB
```

**2. SessionMemory** (rolling summary):
```python
# ÛŒÚ© summary Ø¯Ø± Ù‡Ø± conversation
# Fields:
- conversation (OneToOne to Conversation)
- user (FK)
- cumulative_summary: TEXT (â‰¤150 tokens)
- message_count: INT
- last_updated: DateTime
```

**3. IntentKeyword** (optional):
```python
# Keywords Ø¨Ø±Ø§ÛŒ intent detection
# Fields:
- intent: 'pricing', 'product', 'howto', 'contact', 'general'
- language: 'fa', 'en', 'ar', 'tr'
- keyword: VARCHAR(100)
- weight: FLOAT (1.0-3.0)
- user: FK (nullable - Ø¨Ø±Ø§ÛŒ global keywords)
- is_active: BOOLEAN
```

**4. IntentRouting** (optional):
```python
# Routing config
# Fields:
- intent: PK
- primary_source: 'faq', 'manual', 'products', 'website'
- secondary_sources: ArrayField
- primary_token_budget: INT (default: 800)
- secondary_token_budget: INT (default: 300)
```

#### Ú†Ø·ÙˆØ±ÛŒ:

**Step 1: Edit models.py**
```bash
# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† models Ø¨Ù‡:
src/AI_model/models.py
```

**Step 2: Create migration**
```bash
docker-compose exec web python manage.py makemigrations AI_model

# Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§ÛŒØ¯ Ø¨Ø§Ø´Ù‡:
# Migrations for 'AI_model':
#   AI_model/migrations/0003_tenantknowledge_sessionmemory_intentkeyword_intentrouting.py
#     - Create model TenantKnowledge
#     - Create model SessionMemory
#     - Create model IntentKeyword
#     - Create model IntentRouting
```

**Step 3: Review migration**
```bash
docker-compose exec web python manage.py sqlmigrate AI_model 0003

# Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯:
# - vector fields Ø¯Ø±Ø³Øª Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒØ´Ù†
# - indexes Ø¯Ø±Ø³Øª Ù‡Ø³ØªÙ†
```

**Step 4: Apply migration**
```bash
docker-compose exec web python manage.py migrate AI_model

# Ø®Ø±ÙˆØ¬ÛŒ:
# Running migrations:
#   Applying AI_model.0003_... OK
```

**Step 5: Create vector index manually**
```bash
docker-compose exec db psql -U postgres -d fiko_db

CREATE INDEX idx_tenant_knowledge_tldr_embedding 
ON tenant_knowledge 
USING ivfflat (tldr_embedding vector_cosine_ops) 
WITH (lists = 100);

# Verify:
\d tenant_knowledge

\q
```

#### ØªØ³Øª Ù…ÙˆÙÙ‚ÛŒØª:

```python
docker-compose exec web python manage.py shell

from AI_model.models import TenantKnowledge, SessionMemory
from accounts.models import User

user = User.objects.first()

# Test 1: Create a knowledge chunk
chunk = TenantKnowledge.objects.create(
    user=user,
    chunk_type='faq',
    full_text='Ø§ÛŒÙ† ÛŒÚ© ØªØ³Øª Ø§Ø³Øª',
    tldr='ØªØ³Øª',
    language='fa',
    word_count=4
)
print(f"Created chunk: {chunk.id}")

# Test 2: Add embedding
from AI_model.services.embedding_service import EmbeddingService
emb_service = EmbeddingService(use_cache=True)
embedding = emb_service.get_embedding('ØªØ³Øª', task_type='retrieval_document')

if embedding:
    chunk.tldr_embedding = embedding
    chunk.save()
    print(f"Embedding added! Dimension: {len(embedding)}")

# Test 3: Vector search
from pgvector.django import CosineDistance

query_emb = emb_service.get_embedding('ØªØ³Øª', task_type='retrieval_query')
similar = TenantKnowledge.objects.filter(
    user=user
).order_by(CosineDistance('tldr_embedding', query_emb))[:5]

print(f"Found {similar.count()} similar chunks")

# Success! âœ…
```

#### Ø®Ø±ÙˆØ¬ÛŒ ÙØ§Ø² 1:
- âœ… 4 model Ø¬Ø¯ÛŒØ¯ Ø¯Ø± database
- âœ… Vector index Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù‡
- âœ… Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒÙ… vector search Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯ÛŒÙ…

---

### **ÙØ§Ø² 2: Core Services** (6-8 Ø³Ø§Ø¹Øª)

#### Ú†ÛŒÚ©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:
4 service Ø¬Ø¯ÛŒØ¯ Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³ÛŒÙ…

#### Services:

**1. QueryRouter** (`src/AI_model/services/query_router.py`):
- Intent classification Ø¨Ø§ keyword matching
- Ø±ÙˆÛŒ Ú©Ù„Ù…Ø§Øª ÙØ§Ø±Ø³ÛŒØŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒØŒ Ø¹Ø±Ø¨ÛŒØŒ ØªØ±Ú©ÛŒ
- Ø®Ø±ÙˆØ¬ÛŒ: intent + confidence + primary_source

**2. ContextRetriever** (`src/AI_model/services/context_retriever.py`):
- Vector search Ø¨Ø§ pgvector
- Token budget enforcement
- Ø®Ø±ÙˆØ¬ÛŒ: primary items + secondary items (Ø§Ú¯Ù‡ Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ù‡)

**3. TokenBudgetController** (`src/AI_model/services/token_budget_controller.py`):
- **âš ï¸ CRITICAL:** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² token counter Ø¯Ù‚ÛŒÙ‚ (tiktoken ÛŒØ§ Gemini API metadata)
- Strict 1500 token limit
- Trim Ú©Ø±Ø¯Ù† components
- Ø§ÙˆÙ„ÙˆÛŒØª: system > user query > conversation > primary > secondary

**Ù†Ú©ØªÙ‡ Ù…Ù‡Ù… Token Counting:**
```python
# âŒ Ø§Ø´ØªØ¨Ø§Ù‡: ØªØ®Ù…ÛŒÙ† Ø³Ø§Ø¯Ù‡
def _count_tokens(text):
    return int(len(text.split()) * 1.3)  # Ù†Ø§Ø¯Ù‚ÛŒÙ‚!

# âœ… Ø¯Ø±Ø³Øª: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² tiktoken
import tiktoken

def _count_tokens_accurate(text, model="gpt-3.5-turbo"):
    try:
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except:
        # Fallback: Gemini API metadata
        # ÛŒØ§ ØªØ®Ù…ÛŒÙ† Ù…Ø­Ø§ÙØ¸Ù‡â€ŒÚ©Ø§Ø±Ø§Ù†Ù‡
        return int(len(text.split()) * 1.5)  # Ø¨ÛŒØ´ØªØ± ØªØ®Ù…ÛŒÙ† Ù…ÛŒâ€ŒØ²Ù†ÛŒÙ…
```

**4. SessionMemoryManager** (`src/AI_model/services/session_memory_manager.py`):
- **âš ï¸ CRITICAL:** Rolling summary (Ù‡Ø± 5 Ù¾ÛŒØ§Ù… update) - REPLACE Ù†Ù‡ APPEND
- Gemini Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ Ø¨Ø±Ø§ÛŒ summarization
- Ø®Ø±ÙˆØ¬ÛŒ: context string (summary + recent messages)

**Ù†Ú©ØªÙ‡ Ù…Ù‡Ù… Rolling Summary:**
```python
# âŒ Ø§Ø´ØªØ¨Ø§Ù‡: Append Ú©Ø±Ø¯Ù† summaries (Ù…Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ!)
def _update_summary_wrong(self, session_memory, new_messages):
    old_summary = session_memory.cumulative_summary
    new_part = self._summarize(new_messages)
    session_memory.cumulative_summary = old_summary + "\n" + new_part  # âŒ Ø¬Ù…Ø¹ Ù…ÛŒØ´Ù‡!
    session_memory.save()

# âœ… Ø¯Ø±Ø³Øª: Replace Ú©Ø±Ø¯Ù† summary
def _update_summary_correct(self, session_memory, conversation):
    all_messages = Message.objects.filter(conversation=conversation)
    
    if not session_memory.cumulative_summary:
        # Ø§ÙˆÙ„ÛŒÙ† Ø¨Ø§Ø±: Ø®Ù„Ø§ØµÙ‡ Ù‡Ù…Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
        new_summary = self._summarize_all(all_messages)
    else:
        # Ø¯ÙØ¹Ø§Øª Ø¨Ø¹Ø¯: Ø®Ù„Ø§ØµÙ‡ (summary Ù‚Ø¨Ù„ÛŒ + Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯)
        prompt = f"""Previous summary: {session_memory.cumulative_summary}
        
New messages: {new_messages_text}

Update the summary (REPLACE the old one, max 50 words):"""
        new_summary = self._call_gemini(prompt)
    
    session_memory.cumulative_summary = new_summary  # âœ… Replace
    session_memory.message_count = all_messages.count()
    session_memory.save()
```

#### Ú†Ø·ÙˆØ±ÛŒ:

**Step 1-4: Create service files**
```bash
# Ø¯Ø± src/AI_model/services/
touch query_router.py
touch context_retriever.py
touch token_budget_controller.py
touch session_memory_manager.py
```

**Ú©Ø¯ Ù‡Ø± service Ø±Ùˆ Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³ÛŒÙ… (Ø·Ø¨Ù‚ Ø·Ø±Ø§Ø­ÛŒ Ø¯Ø± Ø³Ù†Ø¯ v2.1)**

#### ØªØ³Øª Ù…ÙˆÙÙ‚ÛŒØª:

```python
docker-compose exec web python manage.py shell

from AI_model.services.query_router import QueryRouter
from AI_model.services.context_retriever import ContextRetriever
from AI_model.services.token_budget_controller import TokenBudgetController
from AI_model.services.session_memory_manager import SessionMemoryManager
from AI_model.services.gemini_service import GeminiChatService
from accounts.models import User

user = User.objects.first()

# Test 1: QueryRouter
result = QueryRouter.route_query("Ù‚ÛŒÙ…Øª Ù¾Ù„Ù† Pro Ú†Ù‚Ø¯Ø±Ù‡ØŸ", user)
print(f"Intent: {result['intent']}")  # Ø¨Ø§ÛŒØ¯ 'pricing' Ø¨Ø§Ø´Ù‡
print(f"Primary source: {result['primary_source']}")  # Ø¨Ø§ÛŒØ¯ 'faq' Ø¨Ø§Ø´Ù‡
print(f"Confidence: {result['confidence']}")

# Test 2: TokenBudgetController
components = {
    'system_prompt': 'You are a helpful AI assistant.' * 50,  # Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯!
    'user_query': 'Ù‚ÛŒÙ…Øª Ú†Ù‚Ø¯Ø±Ù‡ØŸ',
    'conversation': 'Customer: Ø³Ù„Ø§Ù…\nAssistant: Ø³Ù„Ø§Ù…',
    'primary_context': [
        {'title': 'FAQ 1', 'content': 'Ù…Ø­ØªÙˆØ§' * 500}  # Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯!
    ],
    'secondary_context': []
}

trimmed = TokenBudgetController.trim_to_budget(components)
print(f"Total tokens: {trimmed['total_tokens']}")  # Ø¨Ø§ÛŒØ¯ â‰¤1500 Ø¨Ø§Ø´Ù‡

# Test 3: SessionMemoryManager
gemini_service = GeminiChatService(user)
memory_manager = SessionMemoryManager(gemini_service)

# Ø¨Ø§ÛŒØ¯ Ø¨Ø¯ÙˆÙ† error Ú©Ø§Ø± Ú©Ù†Ù‡
context = memory_manager.get_memory_context(None)
print(f"Context length: {len(context)}")

# Success! âœ…
```

#### Ø®Ø±ÙˆØ¬ÛŒ ÙØ§Ø² 2:
- âœ… QueryRouter Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù‡
- âœ… TokenBudgetController enforce Ù…ÛŒâ€ŒÚ©Ù†Ù‡
- âœ… SessionMemoryManager Ø¢Ù…Ø§Ø¯Ù‡
- âœ… ContextRetriever Ø¢Ù…Ø§Ø¯Ù‡ (Ø¨Ø¹Ø¯ Ø§Ø² indexing ØªØ³Øª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…)

---

### **ÙØ§Ø² 3: Refactor GeminiChatService** (2-3 Ø³Ø§Ø¹Øª)

#### Ú†ÛŒÚ©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:
Ù…ØªØ¯ `_build_prompt()` Ø±Ùˆ Ú©Ø§Ù…Ù„ refactor Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…

#### Ù‚Ø¨Ù„ (ÙØ¹Ù„ÛŒ):
```python
def _build_prompt(self, customer_message, conversation):
    # Ù‡Ù…Ù‡ manual_prompt
    # Ù‡Ù…Ù‡ FAQ (8 items)
    # Ù‡Ù…Ù‡ Products (6 items)
    # Ù‡Ù…Ù‡ Website (10 pages)
    # â†’ 24,000+ tokens!
```

#### Ø¨Ø¹Ø¯ (Ø¬Ø¯ÛŒØ¯):
```python
def _build_prompt(self, customer_message, conversation):
    # 1. Intent classification
    routing = QueryRouter.route_query(customer_message, self.user)
    
    # 2. Vector search (ÙÙ‚Ø· Ù…Ø±ØªØ¨Ø·â€ŒØªØ±ÛŒÙ†â€ŒÙ‡Ø§)
    retriever = ContextRetriever(self.user)
    context_data = retriever.retrieve(customer_message, routing)
    
    # 3. Rolling summary
    memory_manager = SessionMemoryManager(self)
    conversation_context = memory_manager.get_memory_context(conversation)
    
    # 4. Token budget enforcement
    components = {...}
    trimmed = TokenBudgetController.trim_to_budget(components)
    
    # 5. Build minimal prompt
    # â†’ â‰¤1500 tokens! âœ…
```

#### Ú†Ø·ÙˆØ±ÛŒ:

**Step 1: Backup Ù‚Ø¯ÛŒÙ…ÛŒ**
```bash
cp src/AI_model/services/gemini_service.py src/AI_model/services/gemini_service.py.backup
```

**Step 2: Refactor _build_prompt()**
```python
# Replace Ú©Ø±Ø¯Ù† Ù…ØªØ¯ _build_prompt Ø¨Ø§ implementation Ø¬Ø¯ÛŒØ¯
```

**Step 3: Ø­ÙØ¸ compatibility**
```python
# Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø¯ÛŒÚ¯Ù‡ Ù†Ø¨Ø§ÛŒØ¯ Ø¹ÙˆØ¶ Ø¨Ø´Ù‡:
- generate_response() signature
- create_ai_message()
- API endpoints
- return values
```

#### ØªØ³Øª Ù…ÙˆÙÙ‚ÛŒØª:

```python
docker-compose exec web python manage.py shell

from AI_model.services.gemini_service import GeminiChatService
from message.models import Conversation
from accounts.models import User

user = User.objects.first()
service = GeminiChatService(user)

# Test Ø¨Ø§ ÛŒÚ© Ø³ÙˆØ§Ù„ ÙˆØ§Ù‚Ø¹ÛŒ
response = service.generate_response("Ù‚ÛŒÙ…Øª Ù¾Ù„Ù† Pro Ú†Ù‚Ø¯Ø±Ù‡ØŸ")

print(f"Success: {response['success']}")
print(f"Response: {response['response'][:100]}")
print(f"Tokens: {response['metadata']['total_tokens']}")  # Ø¨Ø§ÛŒØ¯ â‰¤1500 Ø¨Ø§Ø´Ù‡!

# Ø¨Ø§ conversation
conv = Conversation.objects.filter(user=user).first()
if conv:
    response2 = service.generate_response("Ù…Ù…Ù†ÙˆÙ†!", conv)
    print(f"With conversation - Tokens: {response2['metadata']['total_tokens']}")

# Success! âœ…
```

#### Ø®Ø±ÙˆØ¬ÛŒ ÙØ§Ø² 3:
- âœ… _build_prompt() refactored
- âœ… Token usage â‰¤1500
- âœ… API compatibility Ø­ÙØ¸ Ø´Ø¯Ù‡
- âœ… Ù‡Ù…Ù‡ Ú†ÛŒØ² Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù‡

---

### **ÙØ§Ø² 4: Data Indexing** (4-6 Ø³Ø§Ø¹Øª)

#### Ú†ÛŒÚ©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:
Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø±Ùˆ Ø¨Ù‡ TenantKnowledge Ù…Ù†ØªÙ‚Ù„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…

#### Data Sources:

**1. FAQ (QAPair model):**
```
- Ù‡Ø± QAPair â†’ 1 chunk Ø¯Ø± TenantKnowledge
- chunk_type = 'faq'
- full_text = question + answer
- tldr = question (Ú©ÙˆØªØ§Ù‡â€ŒØªØ±)
- Generate embedding Ø¨Ø±Ø§ÛŒ tldr
```

**2. Manual Prompt:**
```
- Ø§Ú¯Ù‡ < 1000 Ú©Ù„Ù…Ù‡ â†’ 1 chunk
- Ø§Ú¯Ù‡ > 1000 Ú©Ù„Ù…Ù‡ â†’ chunk Ú©Ù†ÛŒÙ… Ø¨Ù‡ paragraphs
- Ø¨Ø±Ø§ÛŒ Ù‡Ø± chunk Ø¨Ø²Ø±Ú¯ â†’ TL;DR Ø¨Ø§ Gemini
- Generate embeddings
```

**3. Products:**
```
- Ù‡Ø± Product â†’ 1 chunk
- chunk_type = 'product'
- full_text = title + description + price
- Generate embeddings
```

**4. Website Pages:**
```
- Ù‡Ø± WebsitePage â†’ 1 ÛŒØ§ Ú†Ù†Ø¯ chunk
- Ø§Ú¯Ù‡ cleaned_content Ø¨Ø²Ø±Ú¯Ù‡ â†’ chunk Ú©Ù†ÛŒÙ…
- Generate embeddings
```

#### Ú†Ø·ÙˆØ±ÛŒ:

**Step 1: Create management command**
```bash
mkdir -p src/AI_model/management/commands
touch src/AI_model/management/commands/index_tenant_knowledge.py
```

**Step 2: Implementation**
```python
# Command structure:
class Command(BaseCommand):
    def add_arguments(self, parser):
        parser.add_argument('--user', type=str, help='Username')
        parser.add_argument('--chunk-size', type=int, default=400)
        parser.add_argument('--dry-run', action='store_true')
    
    def handle(self, *args, **options):
        user = User.objects.get(username=options['user'])
        
        # 1. Index FAQs
        self.index_faqs(user)
        
        # 2. Index Manual Prompt
        self.index_manual_prompt(user, options['chunk_size'])
        
        # 3. Index Products
        self.index_products(user)
        
        # 4. Index Website Pages
        self.index_website_pages(user)
```

**Step 3: Run command**
```bash
# Test Ø¨Ø§ ÛŒÚ© user
docker-compose exec web python manage.py index_tenant_knowledge --user admin --dry-run

# ÙˆØ§Ù‚Ø¹ÛŒ:
docker-compose exec web python manage.py index_tenant_knowledge --user admin

# Ø®Ø±ÙˆØ¬ÛŒ:
# âœ… Indexed 100 FAQ pairs
# âœ… Indexed 45 manual prompt chunks (generated 45 TL;DRs)
# âœ… Indexed 200 products
# âœ… Indexed 150 website page chunks
# âœ… Generated 495 embeddings
# âœ… Total time: 8m 32s
```

#### ØªØ³Øª Ù…ÙˆÙÙ‚ÛŒØª:

```python
docker-compose exec web python manage.py shell

from AI_model.models import TenantKnowledge
from accounts.models import User

user = User.objects.first()

# Check counts
print(f"FAQ: {TenantKnowledge.objects.filter(user=user, chunk_type='faq').count()}")
print(f"Manual: {TenantKnowledge.objects.filter(user=user, chunk_type='manual').count()}")
print(f"Product: {TenantKnowledge.objects.filter(user=user, chunk_type='product').count()}")
print(f"Website: {TenantKnowledge.objects.filter(user=user, chunk_type='website').count()}")
print(f"Total: {TenantKnowledge.objects.filter(user=user).count()}")

# Test embeddings
chunks_with_emb = TenantKnowledge.objects.filter(
    user=user,
    tldr_embedding__isnull=False
).count()
print(f"Chunks with embeddings: {chunks_with_emb}")

# Test search
from AI_model.services.context_retriever import ContextRetriever
from AI_model.services.query_router import QueryRouter

routing = QueryRouter.route_query("Ù‚ÛŒÙ…Øª Ú†Ù‚Ø¯Ø±Ù‡ØŸ", user)
retriever = ContextRetriever(user)
context = retriever.retrieve("Ù‚ÛŒÙ…Øª Ú†Ù‚Ø¯Ø±Ù‡ØŸ", routing)

print(f"Retrieved {len(context['primary']['items'])} primary items")
print(f"First item: {context['primary']['items'][0]['title']}")

# Success! âœ…
```

#### Ø®Ø±ÙˆØ¬ÛŒ ÙØ§Ø² 4:
- âœ… Ù‡Ù…Ù‡ data indexed Ø´Ø¯Ù‡
- âœ… Embeddings generated
- âœ… Vector search Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù‡
- âœ… Context retrieval Ù…ÙˆÙÙ‚

---

### **ÙØ§Ø² 5: Integration Testing** (2-3 Ø³Ø§Ø¹Øª)

#### Ú†ÛŒÚ©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:
Ú©Ù„ Ø³ÛŒØ³ØªÙ… Ø±Ùˆ end-to-end ØªØ³Øª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…

#### Test Cases:

**Test 1: Intent Classification**
```python
test_queries = [
    ("Ù‚ÛŒÙ…Øª Ù¾Ù„Ù† Pro Ú†Ù‚Ø¯Ø±Ù‡ØŸ", "pricing", "faq"),
    ("Ù…Ø­ØµÙˆÙ„Ø§Øª Ø´Ù…Ø§ Ú†ÛŒ Ù‡Ø³ØªÙ†ØŸ", "product", "products"),
    ("Ú†Ø·ÙˆØ± Ø«Ø¨Øª Ù†Ø§Ù… Ú©Ù†Ù…ØŸ", "howto", "manual"),
    ("Ø´Ù…Ø§Ø±Ù‡ ØªÙ…Ø§Ø³ Ú†ÛŒÙ‡ØŸ", "contact", "manual"),
]

for query, expected_intent, expected_source in test_queries:
    result = QueryRouter.route_query(query, user)
    assert result['intent'] == expected_intent
    assert result['primary_source'] == expected_source
    print(f"âœ… {query} â†’ {result['intent']}")
```

**Test 2: Token Budget**
```python
# Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø®ØªÙ„Ù
queries = [
    "Ø³Ù„Ø§Ù…",
    "Ù‚ÛŒÙ…Øª Ú†Ù‚Ø¯Ø±Ù‡ØŸ",
    "Ù…ÛŒâ€ŒØ®ÙˆØ§Ù… Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø´Ù…Ø§ Ø¨ÛŒØ´ØªØ± Ø¨Ø¯ÙˆÙ†Ù… Ùˆ Ø¨Ø¨ÛŒÙ†Ù… Ú©Ø¯ÙˆÙ… Ø¨Ø±Ø§ÛŒ Ú©Ø³Ø¨ Ùˆ Ú©Ø§Ø± Ù…Ù† Ø¨Ù‡ØªØ±Ù‡",
]

for query in queries:
    response = service.generate_response(query)
    tokens = response['metadata']['total_tokens']
    assert tokens <= 1500, f"Token limit exceeded: {tokens}"
    print(f"âœ… Query: {query[:30]}... â†’ {tokens} tokens")
```

**Test 3: Response Quality**
```python
# Ø³ÙˆØ§Ù„Ø§Øª Ù…Ø¹Ù…ÙˆÙ„
test_qa = [
    ("Ù‚ÛŒÙ…Øª Ù¾Ù„Ù† Pro Ú†Ù‚Ø¯Ø±Ù‡ØŸ", ["Ù‚ÛŒÙ…Øª", "Ù¾Ù„Ù†", "Pro"]),  # Ø¨Ø§ÛŒØ¯ Ø§ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø±Ùˆ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù‡
    ("Ú†Ø·ÙˆØ± Ø«Ø¨Øª Ù†Ø§Ù… Ú©Ù†Ù…ØŸ", ["Ø«Ø¨Øª Ù†Ø§Ù…", "Ø±Ø§Ù‡Ù†Ù…Ø§"]),
    ("ØªÙ…Ø§Ø³", ["Ø´Ù…Ø§Ø±Ù‡", "Ø§ÛŒÙ…ÛŒÙ„", "ØªÙ…Ø§Ø³"]),
]

for query, expected_keywords in test_qa:
    response = service.generate_response(query)
    response_text = response['response'].lower()
    
    found = sum(1 for kw in expected_keywords if kw in response_text)
    print(f"Query: {query}")
    print(f"  Keywords found: {found}/{len(expected_keywords)}")
    print(f"  Response: {response_text[:100]}...")
```

**Test 4: Conversation Context**
```python
# Test Ø¨Ø§ conversation
conv = Conversation.objects.create(user=user, customer=customer)

# Message 1
r1 = service.generate_response("Ø³Ù„Ø§Ù…", conv)
Message.objects.create(conversation=conv, customer=customer, type='customer', content='Ø³Ù„Ø§Ù…')
Message.objects.create(conversation=conv, customer=customer, type='AI', content=r1['response'])

# Message 2
r2 = service.generate_response("Ù‚ÛŒÙ…Øª Ú†Ù‚Ø¯Ø±Ù‡ØŸ", conv)
Message.objects.create(conversation=conv, customer=customer, type='customer', content='Ù‚ÛŒÙ…Øª Ú†Ù‚Ø¯Ø±Ù‡ØŸ')

# Message 6 (Ø¨Ø§ÛŒØ¯ summary trigger Ø¨Ø´Ù‡)
for i in range(4):
    Message.objects.create(conversation=conv, customer=customer, type='customer', content=f'test {i}')

r3 = service.generate_response("Ø®Ù„Ø§ØµÙ‡ Ø¨Ú¯Ùˆ", conv)

# Check SessionMemory
from AI_model.models import SessionMemory
memory = SessionMemory.objects.get(conversation=conv)
print(f"Summary: {memory.cumulative_summary}")
print(f"Message count: {memory.message_count}")
```

**Test 5: Performance**
```python
import time

queries = ["Ù‚ÛŒÙ…Øª Ú†Ù‚Ø¯Ø±Ù‡ØŸ"] * 10

start = time.time()
for query in queries:
    response = service.generate_response(query)
end = time.time()

avg_time = (end - start) / len(queries)
print(f"Average response time: {avg_time:.2f}s")
assert avg_time < 10, "Too slow!"
```

#### Ø®Ø±ÙˆØ¬ÛŒ ÙØ§Ø² 5:
- âœ… Ù‡Ù…Ù‡ test cases pass
- âœ… Token budget Ù…Ø­ØªØ±Ù…
- âœ… Response quality Ø®ÙˆØ¨
- âœ… Performance Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„

---

### **ÙØ§Ø² 6: API Compatibility Check** (1 Ø³Ø§Ø¹Øª)

#### Ú†ÛŒÚ©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:
Ù…Ø·Ù…Ø¦Ù† Ù…ÛŒâ€ŒØ´ÛŒÙ… frontend Ù‡Ù…Ú†Ù†Ø§Ù† Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù‡

#### Test:

**Test 1: Ask API**
```bash
curl -X POST http://localhost:8000/api/v1/ai/ask/ \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Ù‚ÛŒÙ…Øª Ú†Ù‚Ø¯Ø±Ù‡ØŸ",
    "conversation_id": "CONV_ID"
  }'

# Response Ø¨Ø§ÛŒØ¯ Ø¨Ø§Ø´Ù‡:
{
  "success": true,
  "response": "...",
  "response_time_ms": 6500,
  "metadata": {
    "total_tokens": 1450,  # â‰¤1500 âœ…
    "model_used": "gemini-1.5-flash"
  }
}
```

**Test 2: Automatic Response (via Signal)**
```python
# Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… customer Ø§Ø² Telegram/Instagram
# Ø¨Ø§ÛŒØ¯ Ø®ÙˆØ¯Ú©Ø§Ø± AI Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡
# Ùˆ Ø¯Ø± Message model Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø´Ù‡
```

**Test 3: WebSocket**
```javascript
// Ø§Ø² frontend
ws.send(JSON.stringify({
  type: 'chat_message',
  content: 'Ø³Ù„Ø§Ù…',
  message_type: 'customer'
}));

// Ø¨Ø§ÛŒØ¯ AI response Ø±Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†Ù‡
```

#### Ø®Ø±ÙˆØ¬ÛŒ ÙØ§Ø² 6:
- âœ… Ù‡Ù…Ù‡ API endpoints Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù†
- âœ… Frontend compatibility Ø­ÙØ¸ Ø´Ø¯Ù‡
- âœ… WebSocket Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù‡

---

### **ÙØ§Ø² 7: Production Deployment** (2 Ø³Ø§Ø¹Øª)

#### Checklist:

```bash
# 1. Commit changes
git add .
git commit -m "feat: Implement Lean RAG v2.1 - 86% token reduction"

# 2. Push to repository
git push origin main

# 3. Ø¯Ø± server:
cd /path/to/Fiko-Backend
git pull origin main

# 4. Install dependencies
docker-compose exec web pip install -r src/requirements/base.txt

# ÛŒØ§ rebuild:
docker-compose build web

# 5. Setup pgvector
docker-compose exec db psql -U postgres -d fiko_db
CREATE EXTENSION IF NOT EXISTS vector;
\q

# 6. Run migrations
docker-compose exec web python manage.py migrate

# 7. Create vector index
docker-compose exec db psql -U postgres -d fiko_db
CREATE INDEX idx_tenant_knowledge_tldr_embedding 
ON tenant_knowledge 
USING ivfflat (tldr_embedding vector_cosine_ops) 
WITH (lists = 100);
\q

# 8. Index data Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ù…ÙˆØ¬ÙˆØ¯
docker-compose exec web python manage.py index_tenant_knowledge --user USER1
docker-compose exec web python manage.py index_tenant_knowledge --user USER2

# 9. Restart services
docker-compose restart web celery_worker

# 10. Monitor logs
docker-compose logs -f web celery_worker | grep -i "token\|rag\|intent"

# 11. Test Ø¯Ø± production
# Ø§Ø±Ø³Ø§Ù„ Ú†Ù†Ø¯ Ù¾ÛŒØ§Ù… test Ùˆ Ø¨Ø±Ø±Ø³ÛŒ:
# - Token usage
# - Response quality
# - Latency
```

---

## ğŸ“Š Success Metrics

Ø¨Ø¹Ø¯ Ø§Ø² deploymentØŒ Ø§ÛŒÙ† metrics Ø±Ùˆ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:

```python
# Ø¯Ø± Django admin ÛŒØ§ shell:
from AI_model.models import AIUsageTracking
from datetime import date, timedelta

# Usage Ø§Ù…Ø±ÙˆØ²
today_usage = AIUsageTracking.objects.filter(
    date=date.today()
).aggregate(
    total_tokens=Sum('total_tokens'),
    total_requests=Sum('total_requests'),
    avg_response_time=Avg('average_response_time_ms')
)

# Calculate per-conversation average
avg_tokens_per_conv = today_usage['total_tokens'] / today_usage['total_requests']

print(f"Average tokens/conversation: {avg_tokens_per_conv}")  # Ø¨Ø§ÛŒØ¯ â‰¤1500 Ø¨Ø§Ø´Ù‡
print(f"Average response time: {today_usage['avg_response_time']}ms")  # Ø¨Ø§ÛŒØ¯ <10s Ø¨Ø§Ø´Ù‡
```

**Target Metrics:**
- âœ… Average input tokens: â‰¤1500 (vs 11,000 Ù‚Ø¨Ù„ÛŒ)
- âœ… Average response time: <8s (vs 15-20s Ù‚Ø¨Ù„ÛŒ)
- âœ… Cost per conversation: ~$0.03 (vs $0.28 Ù‚Ø¨Ù„ÛŒ)

---

## â±ï¸ Timeline Summary

| ÙØ§Ø² | Ù…Ø¯Øª Ø²Ù…Ø§Ù† | Ø®Ø±ÙˆØ¬ÛŒ Ø§ØµÙ„ÛŒ |
|-----|---------|-----------|
| **0. Infrastructure** | 2-3 Ø³Ø§Ø¹Øª | pgvector Ø¢Ù…Ø§Ø¯Ù‡ |
| **1. Models** | 3-4 Ø³Ø§Ø¹Øª | 4 model + vector index |
| **2. Services** | 6-8 Ø³Ø§Ø¹Øª | 4 service Ø¬Ø¯ÛŒØ¯ |
| **3. Refactor** | 2-3 Ø³Ø§Ø¹Øª | _build_prompt() Ø¬Ø¯ÛŒØ¯ |
| **4. Indexing** | 4-6 Ø³Ø§Ø¹Øª | Data indexed |
| **5. Testing** | 2-3 Ø³Ø§Ø¹Øª | Ù‡Ù…Ù‡ tests pass |
| **6. API Check** | 1 Ø³Ø§Ø¹Øª | Frontend Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù‡ |
| **7. Deployment** | 2 Ø³Ø§Ø¹Øª | Production ready |
| **Total** | **22-30 Ø³Ø§Ø¹Øª** | **3-4 Ø±ÙˆØ² Ú©Ø§Ø±ÛŒ** |

---

## â“ Ø³ÙˆØ§Ù„Ø§Øª Ø¨Ø±Ø§ÛŒ ØªØ§ÛŒÛŒØ¯

Ù‚Ø¨Ù„ Ø§Ø² Ø´Ø±ÙˆØ¹ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ù… Ø¨Ø¯ÙˆÙ†Ù…:

1. **pgvector version:** PostgreSQL Ø´Ù…Ø§ version Ú†Ù†Ø¯Ù‡ØŸ (Ø¨Ø§ÛŒØ¯ 11+ Ø¨Ø§Ø´Ù‡)
   ```bash
   docker-compose exec db psql -U postgres -c "SELECT version();"
   ```

2. **Test users:** Ú†Ù†Ø¯ ØªØ§ user Ø¯Ø§Ø±ÛŒØ¯ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¨Ø±Ø§Ø´ÙˆÙ† index Ø¨Ø²Ù†ÛŒÙ…ØŸ

3. **Manual prompt size:** Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† manual_prompt Ú†Ù†Ø¯ Ú©Ù„Ù…Ù‡ Ù‡Ø³ØªØŸ
   ```python
   from settings.models import AIPrompts
   max_words = max(
       len(p.manual_prompt.split()) 
       for p in AIPrompts.objects.all() 
       if p.manual_prompt
   )
   print(f"Max manual_prompt: {max_words} words")
   ```

4. **Downtime tolerance:** Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒÙ… Ø¨Ø±Ø§ÛŒ migration Ú†Ù†Ø¯ Ø¯Ù‚ÛŒÙ‚Ù‡ downtime Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…ØŸ ÛŒØ§ Ø¨Ø§ÛŒØ¯ zero-downtime Ø¨Ø§Ø´Ù‡?

5. **Backup:** Ø¢ÛŒØ§ Ù‚Ø¨Ù„ Ø§Ø² migration backup Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒØ¯ØŸ

---

## ğŸš€ Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ØŸ

Ø§Ú¯Ù‡ Ø§ÛŒÙ† Ù¾Ù„Ù† OK Ù‡Ø³ØªØŒ Ù…ÛŒâ€ŒØªÙˆÙ†Ù… **Ø§Ø² ÙØ§Ø² 0 Ø´Ø±ÙˆØ¹ Ú©Ù†Ù…**.

**Ù…ÙˆØ§ÙÙ‚ÛŒØ¯ØŸ** âœ…

