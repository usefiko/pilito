"""
Test script for Persian-aware chunking
Run: python manage.py shell < test_persian_chunker.py
"""

def test_persian_chunker():
    """Test Persian chunker with real Persian content"""
    from AI_model.services.persian_chunker import PersianChunker
    
    print("=" * 80)
    print("ðŸ§ª Testing Persian-Aware Chunking")
    print("=" * 80)
    
    # Test 1: Persian text detection
    print("\n1ï¸âƒ£ Test: Language Detection")
    persian_text = "Ø¨ÙˆØ±Ø³ÛŒÙ‡ Ú©ÙˆÚ†ÛŒÙ†Ú¯ ÙØ±Ø§Ú©ÙˆÚ† Ø¨Ø±Ø§ÛŒ Ø§ÙØ±Ø§Ø¯ Ø§Ø«Ø±Ú¯Ø°Ø§Ø± Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª"
    english_text = "Hello world this is a test"
    mixed_text = "Hello Ø³Ù„Ø§Ù… world Ø¯Ù†ÛŒØ§"
    
    lang_fa = PersianChunker._detect_language(persian_text)
    lang_en = PersianChunker._detect_language(english_text)
    lang_mixed = PersianChunker._detect_language(mixed_text)
    
    print(f"  Persian text: '{persian_text[:40]}...' â†’ {lang_fa}")
    print(f"  English text: '{english_text}' â†’ {lang_en}")
    print(f"  Mixed text: '{mixed_text}' â†’ {lang_mixed}")
    print(f"  âœ… Language detection working!")
    
    # Test 2: Keyword extraction (Persian)
    print("\n2ï¸âƒ£ Test: Persian Keyword Extraction")
    keywords = PersianChunker._extract_keywords_persian(persian_text)
    print(f"  Text: {persian_text}")
    print(f"  Keywords: {keywords}")
    print(f"  âœ… Extracted {len(keywords)} keywords")
    
    # Test 3: TL;DR (Persian)
    print("\n3ï¸âƒ£ Test: Persian TL;DR")
    long_persian = """
    Ø¨ÙˆØ±Ø³ÛŒÙ‡ Ú©ÙˆÚ†ÛŒÙ†Ú¯ ÙØ±Ø§Ú©ÙˆÚ† Ø¨Ø±Ø§ÛŒ Ø§ÙØ±Ø§Ø¯ Ø§Ø«Ø±Ú¯Ø°Ø§Ø± Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ú©ÙˆÚ†ÛŒÙ†Ú¯ 
    ÙÙ‚Ø· Ø¨Ù‡ Ø±Ø´Ø¯ Ø´Ø®ØµÛŒâ€ŒØ´Ø§Ù† Ø®ØªÙ… Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§ÛŒÙ† Ø§ÙØ±Ø§Ø¯ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø§Ù„Ù‡Ø§Ù…â€ŒØ¨Ø®Ø´ Ø¯ÛŒÚ¯Ø±Ø§Ù† Ø¨Ø§Ø´Ù†Ø¯.
    Ù‡Ø¯Ù Ù…Ø§ ÙÙ‚Ø· Ø¢Ù…ÙˆØ²Ø´ Ù…Ù‡Ø§Ø±Øª Ù†ÛŒØ³Øª Ø¨Ù„Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… ÙØ±Ù‡Ù†Ú¯ Ú©ÙˆÚ†ÛŒÙ†Ú¯ Ø±Ø§ Ø¯Ø± Ø¬Ø§Ù…Ø¹Ù‡ Ú¯Ø³ØªØ±Ø´ Ø¯Ù‡ÛŒÙ….
    Ù…Ø¯Ù„ ØªØ®ØµÛŒØµ Ø¨ÙˆØ±Ø³ÛŒÙ‡ Ù…ØªÙ†Ø§Ø³Ø¨ Ø¨Ø§ Ø´Ø±Ø§ÛŒØ· Ù‡Ø± ÙØ±Ø¯ Ùˆ Ù¾Ø³ Ø§Ø² Ø¨Ø±Ø±Ø³ÛŒ Ø³ÙˆØ§Ø¨Ù‚ ØªØ¹ÛŒÛŒÙ† Ù…ÛŒâ€ŒØ´ÙˆØ¯.
    Ø¯Ø± ØµÙˆØ±Øª Ù¾Ø°ÛŒØ±Ø´ Ø¨Ø®Ø´ÛŒ Ø§Ø² Ù‡Ø²ÛŒÙ†Ù‡ Ø¯ÙˆØ±Ù‡ Ú©Ø³Ø± Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯ Ùˆ Ø¨ÙˆØ±Ø³ÛŒÙ‡â€ŒØ´ÙˆÙ†Ø¯Ù‡ Ø¯Ø± Ù‚Ø§Ù„Ø¨ Ù‡Ù…Ú©Ø§Ø±ÛŒ 
    Ø¨Ø§ Ø¢Ú©Ø§Ø¯Ù…ÛŒ Ù…Ø´Ø§Ø±Ú©Øª Ø®ÙˆØ§Ù‡Ø¯ Ø¯Ø§Ø´Øª. Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ Ø¨Ù‡ ÙØ±Ø§ÛŒÙ†Ø¯ Ø¨Ø§ Ú©Ø§Ø±Ø´Ù†Ø§Ø³Ø§Ù† Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ú¯ÛŒØ±ÛŒØ¯.
    """
    tldr = PersianChunker.extract_tldr_persian(long_persian.strip(), max_words=30)
    print(f"  Original ({len(long_persian.split())} words):")
    print(f"    {long_persian.strip()[:100]}...")
    print(f"  TL;DR ({len(tldr.split())} words):")
    print(f"    {tldr}")
    print(f"  âœ… TL;DR generated successfully")
    
    # Test 4: Chunking with metadata
    print("\n4ï¸âƒ£ Test: Chunking with Metadata")
    chunks_data = PersianChunker.chunk_text_with_metadata(
        text=long_persian.strip(),
        chunk_size=50,  # Small for testing
        overlap=10,
        page_title="Ø¨ÙˆØ±Ø³ÛŒÙ‡ Ú©ÙˆÚ†ÛŒÙ†Ú¯",
        page_url="/scholarship",
        h1_tags=["Ø¨ÙˆØ±Ø³ÛŒÙ‡"],
        h2_tags=["Ø´Ø±Ø§ÛŒØ·", "Ù…Ø²Ø§ÛŒØ§"]
    )
    
    print(f"  Input: {len(long_persian.split())} words")
    print(f"  Output: {len(chunks_data)} chunks")
    for i, (chunk_text, metadata) in enumerate(chunks_data):
        print(f"\n  Chunk {i+1}/{len(chunks_data)}:")
        print(f"    Text: {chunk_text[:80]}...")
        print(f"    Keywords: {metadata.keywords[:3]}")
        print(f"    Language: {metadata.language}")
        print(f"    H1 tags: {metadata.h1_tags}")
    
    print(f"\n  âœ… Chunking with metadata working!")
    
    # Test 5: Token-based chunking (approximation)
    print("\n5ï¸âƒ£ Test: Token-based Chunking")
    tokens = PersianChunker._tokenize_persian(persian_text)
    print(f"  Text: {persian_text}")
    print(f"  Tokens: {len(tokens)} (approximation)")
    print(f"  âœ… Tokenization working!")
    
    print("\n" + "=" * 80)
    print("ðŸŽ‰ All tests passed! Persian chunking is working correctly!")
    print("=" * 80)
    
    return True


# Run tests
if __name__ == "__main__":
    test_persian_chunker()

