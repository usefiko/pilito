services:

  web:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: django_app
    command: gunicorn core.asgi:application -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000 --workers 2 --threads 4 --timeout 120
    working_dir: /app
    volumes:
      - ./src:/app
      - static_volume:/app/static
      - media_volume:/app/media
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      - REDIS_URL=redis://redis:6379
      - RAPIDAPI_KEY=${RAPIDAPI_KEY}
      # Email Configuration - Liara Email SMTP (set EMAIL_BACKEND=django.core.mail.backends.smtp.EmailBackend to enable email sending)
      - EMAIL_BACKEND=django.core.mail.backends.smtp.EmailBackend
      - EMAIL_HOST=smtp.c1.liara.email
      - EMAIL_PORT=587
      - EMAIL_USE_TLS=True
      - EMAIL_USE_SSL=False
      - EMAIL_HOST_USER=zen_torvalds_599nek
      - EMAIL_HOST_PASSWORD=8d071fc6-a36c-43f1-9f09-b25bd408af87
      - EMAIL_TIMEOUT=30
      - DEFAULT_FROM_EMAIL=noreply@mail.pilito.com
      # Google OAuth Configuration (Production)
      - GOOGLE_OAUTH2_REDIRECT_URI=https://api.pilito.com/api/v1/usr/google/callback
      - GOOGLE_OAUTH2_FRONTEND_REDIRECT=https://app.pilito.com/auth/success
    depends_on:
      - db
      - redis

  db:
    image: pgvector/pgvector:pg15
    container_name: postgres_db
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    command: >
      postgres -c shared_preload_libraries='vector'

  redis:
    image: redis:7
    container_name: redis_cache
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  # ========================================
  # ðŸš€ Celery Workers Ø¨Ø§ Priority Queue
  # ========================================
  
  # Worker Ø§ØµÙ„ÛŒ: Ù‡Ù…Ù‡ queue Ù‡Ø§ Ø¨Ø§ Ø§ÙˆÙ„ÙˆÛŒØª
  celery_worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: celery_worker
    command: >
      celery -A core worker
      --loglevel=info
      --concurrency=4
      --prefetch-multiplier=1
      --max-tasks-per-child=50
      --queues=high_priority,default,low_priority
      --hostname=worker_main@%h
    working_dir: /app
    volumes:
      - ./src:/app
      - media_volume:/app/media
    env_file:
      - .env
    environment:
      - REDIS_URL=redis://redis:6379
      - RAPIDAPI_KEY=${RAPIDAPI_KEY}
      - C_FORCE_ROOT=true
    ports:
      - "9808:9808"
    depends_on:
      - db
      - redis
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 1536M
        reservations:
          memory: 512M
  
  # Worker Ø§Ø¶Ø§ÙÛŒ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ AI (High Priority)
  # Ø§ÛŒÙ† worker ÙÙ‚Ø· Ø¨Ù‡ AI tasks Ø±Ø³ÛŒØ¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
  celery_ai:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: celery_ai
    command: >
      celery -A core worker
      --loglevel=info
      --concurrency=2
      --prefetch-multiplier=1
      --max-tasks-per-child=30
      --queues=high_priority
      --hostname=worker_ai@%h
    working_dir: /app
    volumes:
      - ./src:/app
      - media_volume:/app/media
    env_file:
      - .env
    environment:
      - REDIS_URL=redis://redis:6379
      - RAPIDAPI_KEY=${RAPIDAPI_KEY}
      - C_FORCE_ROOT=true
    depends_on:
      - db
      - redis
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1024M
        reservations:
          memory: 256M

  celery_beat:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: celery_beat
    command: celery -A core beat --loglevel=info --scheduler django_celery_beat.schedulers:DatabaseScheduler
    working_dir: /app
    volumes:
      - ./src:/app
    env_file:
      - .env
    environment:
      - REDIS_URL=redis://redis:6379
      - RAPIDAPI_KEY=${RAPIDAPI_KEY}
    depends_on:
      - db
      - redis
      - celery_worker
    restart: unless-stopped

  # Monitoring Stack
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/prometheus/alerts.yml:/etc/prometheus/alerts.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    restart: unless-stopped
    depends_on:
      - web

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3001
      - GF_INSTALL_PLUGINS=redis-datasource
    ports:
      - "3001:3000"
    restart: unless-stopped
    depends_on:
      - prometheus

  redis_exporter:
    image: oliver006/redis_exporter:latest
    container_name: redis_exporter
    command:
      - '--redis.addr=redis://redis:6379'
    ports:
      - "9121:9121"
    restart: unless-stopped
    depends_on:
      - redis

  postgres_exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: postgres_exporter
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}?sslmode=disable"
    ports:
      - "9187:9187"
    restart: unless-stopped
    depends_on:
      - db

volumes:
  postgres_data:
  redis_data:
  static_volume:
  media_volume:
  prometheus_data:
  grafana_data: